# **三、有监督微调**

## 环境准备

### 基础环境

- 操作系统：Ubuntu 22.04
- Miniconda3: conda 24.1.2
- 根据服务器网络情况配置好conda源和pip源（本案例中均使用默认源）

其它部分同第一章所示。

## **模型部署**

本部分选择的模型为`Qwen2.5-1.5B-Instruct`，也可以沿用选择第一章部署的`Qwen2.5-0.5B-Instruct` ，参数量小的模型微调速度会更快一些，电脑性能稍弱或者想节约时间的同学可以选择0.5b，微调过程相同。

`Qwen2.5-1.5B-Instruct`下载

```bash
#选择你的模型保存目录
cd yourmodles

#下载模型
modelscope download --model Qwen/Qwen2.5-1.5B-Instruct
```

## **数据准备**

我们选择有监督微调数据集，如1.1.2 的数据格式所示，以下列出我们可选择的数据集（[示例来源于魏斯博师哥的大模型入门博客](https://www.cnblogs.com/whisper1999/p/18054600)）：

| 序号 | 数据集                                  | 简介                                                         | 数据量 |
| ---- | --------------------------------------- | ------------------------------------------------------------ | ------ |
| 1    | Chinese_medical_dialogue_six_department | 中文医疗问答数据集，包括男科、内科、妇产科、肿瘤科、儿科、外科六个科室的问题。 | 792K   |
| 2    | HuatuoGPT2_sft_instruct_GPT4            | 华佗GPT（HuatuoGPT）第二版训练数据集。                       | 50K    |
| 3    | ChatMed_Consult-v0.3                    | 中文医疗在线问诊数据集ChatMed_Consult_Dataset的50w+在线问诊+ChatGPT回复。 | 500K   |
| 4    | ChatMed_TCM-v0.2                        | 以开源的中医药知识图谱为基础，采用以实体为中心的自指令方法(entity-centric self-instruct)，调用ChatGPT得到11w+的围绕中医药的指令数据。 | 110K   |
| 5    | QiZhen_sft_20k                          | 包含20k训练数据（该数据集来自于启真医学知识库收集整理的真实医患知识问答数据以及在启真医学知识库的药品文本知识基础上，通过对半结构化数据设置特定的问题模板构造的指令数据）。 | 20K    |
| 6    | Huatuo_Lite                             | Huatuo-Lite 是在Huatuo26M数据集的基础上经过多次提纯和重写而精炼优化的数据集。它包含了18万个高质量的医疗问答对，并具有医院科室和相关疾病两个额外的数据维度。 | 180K   |
| 7    | ZhongJing_CMtMedQA                      | 仲景SFT训练集。                                              | 70K    |
| 8    | DISC-Med-SFT_released                   | 包含了超过47万个衍生于现有的医疗数据集重新构建得到的样本。采用了目标导向的策略，通过对于精心选择的几个数据源进行重构来得到SFT数据集。这些数据的作用在于帮助模型学习医疗领域知识，将行为模式与人类偏好对齐，并对齐真实世界在线医疗对话的分布情况。 | 514K   |
| 9    | SZY_TCM_QA                              | 私有数据集。                                                 | 12K    |

我们挑选其中较小的数据集进行训练

```bash
git clone https://www.modelscope.cn/datasets/xiaofengalg/qizen-gpt-sample-sft-20k.git
```

## 编写微调脚本

大模型的微调方式可以分为**全参数微调**和**高效参数微调**两种。

1. 全参数微调 

   全参数微调（Full-Param Fine-tuning）指的是在微调过程中，使用全部的预训练参数，而不仅仅是 更新部分参数。这种方法可以充分利用预训练模型的表示能力，同时避免在大量数据上的重新训 练。 全参数微调的重要性不言而喻。由于LLM是一种大规模的预训练模型，其包含的参数数量巨大，因 此重新训练整个模型往往是不现实的。全参数微调允许我们在保持模型性能的同时，仅使用部分数 据对模型进行微调。这使得我们可以更灵活地适应各种不同的任务和领域。 

2. 高效参数微调 

   由于大语言模型参数量十分庞大，当将其应用到下游任务时，微调全部参数需要相当高的算力。参 数高效微调是指微调少量或额外的模型参数，固定大部分预训练模型（LLM）参数，从而大大降低 了计算和存储成本，同时，也能实现与全量参数微调相当的性能。参数高效微调方法甚至在某些情 况下比全量微调效果更好，可以更好地泛化到域外场景。 **LoRA（Low-Rank Adaptation of Large Language Models）**，直译为大语言模型的低阶自适 应。LoRA 的基本原理是冻结预训练好的模型权重参数，在冻结原模型参数的情况下，通过往模型 中加入额外的网络层，并只训练这些新增的网络层参数。由于这些新增参数数量较少，这样不仅微 调的成本显著下降，还能获得和全参数微调类似的效果。

    除了LoRA之外，还有其他几种高效的微调方法，例如适配器微调（Adapter Tuning）和前缀微调 （Prefix Tuning）等。 **适配器微调方法**通过在Transformer架构的每一层——即自注意力模块和多层感知机（MLP）模块 之间引入额外的适配器层（Adapter Layer）来实现。这些适配器层作为新的可训练参数被加入到 模型中，位于原有的残差连接之间。这种方法通过增加少量的参数来使模型适应新的任务，同时尽 量保留原始模型的大部分权重不变，以此减少微调所需的计算资源。不过，适配器层的引入也会导 致模型深度的增加，从而在模型推理阶段引入额外的时间开销。特别是在没有采用模型并行或数据 并行的情况下，这种开销可能会更加明显。 通过这种方式，适配器微调不仅能够有效地降低微调大型模型的成本，还能在一定程度上提高模型 对新任务的适应能力，是一种平衡了性能与效率的微调策略。

    **前缀微调技术**涉及在输入序列的开头添加一段可训练的“软提示”，这段软提示是一系列连续可微的 向量。这种方法允许模型根据这些额外的提示信息来调整其输出，从而更好地适应特定任务。然 而，由于大多数模型对最大输入长度有所限制，增加软提示的参数数量会导致可用于实际任务输入 的空间减少。这意味着，随着软提示大小的增长，模型能够处理的有效输入长度会相应缩短，这可 能会影响模型的整体性能。因此，前缀微调的效果并不总是随着可训练参数数量的增加而线性提 升，反而可能因输入长度的缩减而受到影响

   使用 LoRA 方法训练的 GPT-2、GPT-3 模型在相近数量的可训练参数下， 性能均优于或相当于使用 上述两种微调方法。因此本课程使用LoRA进行高效参数微调。

    **LISA(Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine Tuning)**使用了Layerwise重要性采样优化方法, 旨在模拟LoRA的更新模式，模拟其快速学习过程,  并通过重要性采样避免LoRA有限的低秩表示能力的缺陷。作者假设可以在全参数训练时通过冻结大 部分层的参数来模拟LoRA更新的行为，使其最后的参数迭代范数达到类似的效果。 这个技术可以把全参训练的显存使用降低到之前的三分之一左右。例如，全参训练一个7b模型大约 需要80G显存（相当于一张完整的A100显卡），但使用LISA训练后却可以使显存降低到30G左右， 这使得使用40G A100显卡甚至是24G A10或者RTX 3090成为可能，且它的显存占用更低、训练速 度更快。

`SWIFT`框架提供了部分大模型的微调脚本，可以在我们下载的源码中的*swift/examples/pytorch/llm/scripts*路径中找到这些脚本。如果这些脚本能够满足我们大部分的微调需求，我们可以选择直接对这些脚本进行修改。如果找不到我们需要的脚本，需要我们根据*swift/docs/source/LLM*中的命令行参数文档自行编写训练脚本。

## **全参数微调**

执行以下命令创建并执行全参数微调脚本：

```bash
# 进入script文件夹
cd ~/script/
 # 创建全参微调脚本sft_full.sh
vim sft_full.sh
# 返回上级目录
cd ..
# 执行全参微调
bash ./script/sft_full.sh
```

其中 `sft_full.sh` 内容如下：

```bash
CUDA_VISIBLE_DEVICES=0,1 \
swift sft \
    --model <model_id_or_path> \
    --train_type full \
    --dataset </path/to/qizen-gpt-sample-sft-20k/qizhen-gpt-sft-20k.json> \
    --torch_dtype bfloat16 \
    --num_train_epochs 2 \
    --per_device_train_batch_size 2 \
    --per_device_eval_batch_size 2 \
    --learning_rate 1e-5 \
    --target_modules all-linear \
    --gradient_accumulation_steps 16 \
    --eval_steps 100 \
    --save_steps 100 \
    --weight_decay 0.01 \
    --save_total_limit 2 \
    --logging_steps 5 \
    --max_length 4096 \
    --output_dir output/sft_full \
    --max_grad_norm 0.5 \
    --warmup_ratio 0.03 \
    --dataloader_num_workers 4 \
```

训练完成后，会和增量预训练一样输出保存路径，我们根据保存的位置启动大模型：

```bash
CUDA_VISIBLE_DEVICES=0 swift infer --stream true --model </path/to/output/sft_full/v0-20250610-005734/checkpoint-1236>
```

Qwen2.5-1.5B-Instruct的回答情况：

```
<<<广誉远安宫牛黄丸是什么，有什么作用

广誉远安宫牛黄丸是一种中药，主要用于治疗中风、偏瘫、昏迷等疾病。它的主要成分包括牛黄、麝香、冰片等，具有清热解毒、开窍醒神、活血化瘀等功效。使用时应遵循医生的建议，注意剂量和用法。
```

全参数微调后的回答情况：

```
<<< 广誉远安宫牛黄丸是什么，有什么作用
广誉远安宫牛黄丸为清热解毒、镇惊开窍之要药。主治中风昏迷、痰火扰心所致的神昏谵语、口舌生疮。

广誉远安宫牛黄丸是广誉远制药有限公司在1984年研制成功的中药新制剂，具有镇静安神、清热解毒的功效，适用于中风昏迷、痰火扰心所致的神昏谵语、口舌生疮等症。
```

### **LoRA（Low Rank Adaption）微调**

**LoRA**介绍：

标题: `LoRA: Low-Rank Adaptation of Large Language Models`
地址: [https://arxiv.org/pdf/2106.09685.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2106.09685.pdf)
公司: 微软
Code: [https://github.com/microsoft/Lo](https://link.zhihu.com/?target=https%3A//github.com/microsoft/LoRA)

执行以下命令创建并执行全参数微调脚本：

```bash
# 进入script文件夹
cd ~/script/
# 创建全参微调脚本sft_full.sh
vim sft_lora.sh
# 返回上级目录
cd ..
# 执行LoRA微调
bash ./script/sft_lora.sh
```

 LoRA微调脚本 sft_lora.sh 内容如下：

```bash
CUDA_VISIBLE_DEVICES=0 \
swift sft \
    --model <model_id_or_path> \
    --train_type lora \
    --dataset </path/to/qizen-gpt-sample-sft-20k/qizhen-gpt-sft-20k.json> \
    --torch_dtype bfloat16 \
    --num_train_epochs 1 \
    --per_device_train_batch_size 1 \
    --per_device_eval_batch_size 1 \
    --learning_rate 1e-4 \
    --lora_rank 8 \
    --lora_alpha 32 \
    --target_modules all-linear \
    --gradient_accumulation_steps 16 \
    --eval_steps 100 \
    --save_steps 100 \
    --save_total_limit 2 \
    --logging_steps 5 \
    --max_length 4096 \
    --output_dir output/sft_lora \
    --warmup_ratio 0.05 \
    --dataloader_num_workers 4 \

```

- `--lora_rank`: 默认为`8`. 只有当`sft_type`指定为'lora'时才生效。
- `--lora_alpha`: 默认为`32`. 只有当`sft_type`指定为'lora'时才生效。
- `--lora_dropout`: 默认为`0.05`, 只有当`sft_type`指定为'lora'时才生效。
- `--lora_target_modules` 'ALL'来训练模型的全部线性层。

训练完成后，会和增量预训练一样输出保存路径，我们根据保存的位置启动大模型：



```bash
# 注意LoRA版本需要先将LoRA权重合并
CUDA_VISIBLE_DEVICES=0 swift export --ckpt_dir <path/to/output/sft_lora/v0-20250609-060856/checkpoint-1236> --merge_lora true

# 使用swift部署
# 合并后会在模型保存checkpoint的目录里保存一个checkpoint-xxx-merged目录，这个就是合并后的模
型
CUDA_VISIBLE_DEVICES=0 swift infer --stream true --model </path/to/output/sft_lora/v0-20250609-060856/checkpoint-1236-merged>
```

LoRA微调后的回答情况：

```shell
<<< 广誉远安宫牛黄丸是什么，有什么作用
广誉远安宫牛黄丸为处方中成药，具有清热解毒、镇惊开窍的作用。

用于治疗心神不宁所致的高热抽搐。常出现以下症状：高热、烦躁不安、惊厥抽搐、舌红绛苔黄燥、脉数有力等症状。
```

### 

### **LISA微调**

LiSA介绍：

标题: LISA- Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning
地址: [https://arxiv.org/pdf/2403.17919.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2403.17919.pdf)
学校: 港科大
Code: [https://github.com/OptimalScale](https://link.zhihu.com/?target=https%3A//github.com/OptimalScale/LMFlow)

执行以下命令创建并执行LISA微调脚本：

```bash
# 进入script文件夹
cd ~/script/
# 创建LISA微调脚本sft_lisa.sh
vim sft_lisa.sh
# 返回上级目录
cd ..
# 执行LISA微调
bash ./script/sft_lisa.sh
```

LISA微调脚本 `sft_lisa.sh` 内容如下：

```bash
CUDA_VISIBLE_DEVICES=0,1 \
swift sft \
    --model <model_id_or_path> \
    --train_type full \
    --lisa_activated_layers 2 \
    --lisa_step_interval 20  \
    --dataset <path/to/qizen-gpt-sample-sft-20k/qizhen-gpt-sft-20k.json> \
    --torch_dtype bfloat16 \
    --num_train_epochs 2 \
    --per_device_train_batch_size 2 \
    --per_device_eval_batch_size 2 \
    --learning_rate 1e-5 \
    --target_modules all-linear \
    --gradient_accumulation_steps 16 \
    --eval_steps 100 \
    --save_steps 100 \
    --weight_decay 0.01 \
    --save_total_limit 2 \
    --logging_steps 5 \
    --max_length 4096 \
    --output_dir output/sft_lisa \
    --max_grad_norm 0.5 \
    --warmup_ratio 0.03 \
    --dataloader_num_workers 4 
```

训练完成后，会和增量预训练一样输出保存路径，我们根据保存的位置启动大模型：

```bash
CUDA_VISIBLE_DEVICES=0 swift infer --stream true --model </path/to/output/sft_full/v0-20250610-005734/checkpoint-1236>
```

LiSA微调后的回答情况：

```shell
<<< 广誉远安宫牛黄丸是什么，有什么作用
广誉远安宫牛黄丸为中药制剂，具有清热解毒、镇惊开窍的功效。用于治疗高热神昏、烦躁谵语、癫痫抽搐等症。

安宫牛黄丸的主要成分有冰片、雄黄、朱砂、人工麝香、薄荷脑、水牛角浓缩粉、珍珠母（煅）、羚羊角（去脂）等。其中，冰片、朱砂和雄黄是安宫牛黄丸的君药，它们能清热解毒、镇惊开窍；薄荷脑、水牛角浓缩粉、珍珠母（煅）能清心除烦、平肝息风；羚羊角（去脂）能清热息风、开窍宁神。

本品可以治疗高热神昏、烦躁谵语、癫痫抽搐等症状。此外，安宫牛黄丸还可以用于治疗中暑引起的高热不退、昏迷、烦躁不安、抽搐、舌红绛苔黄燥等症。在服用安宫牛黄丸时应避免饮酒，以免增加药物对肝脏的毒性。如果出现过敏反应，如皮肤瘙痒、皮疹、呼吸困难、恶心、呕吐、腹痛、腹泻、头晕、头痛、心悸、胸闷等症状，应及时停药并就医。
```

可以比较直观的感受，进行微调后的大模型虽然存在回答效果存在差异，但都优于未训练的1.5b模型。

---

![](QLUNLP_logo.png)
