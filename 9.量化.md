# 九、量化

---

模型量化（Quantization）是一种通过降低神经网络权重精度来压缩模型大小的技术。对于大型语言模型（LLM）而言，量化技术尤其重要。研究表明，尽管神经网络的训练和推理过程中某些步骤需要高精度计算，但在许多其他情况下，使用较低精度（如INT8）计算不仅能显著减小模型体积，还能够让模型在硬件性能较弱的设备上运行，同时保持可接受的性能和准确性。

模型推理过程本质上是大量复杂数学运算的组合，尤其依赖于矩阵运算，这恰恰是并行计算的优势所在。尽管单核CPU能够执行多种计算任务并且在处理单一任务时表现较为高效，但其更适合顺序执行的任务。相比之下，GPU（图形处理单元）最初是为图形计算设计的，因此它特别擅长并行处理多个简单任务，尤其是大规模矩阵乘法，而这正是深度学习模型训练和推理过程中的常见计算操作。

然而，将整个模型加载到GPU中往往会大幅提高显存的占用量。尤其是对于大型语言模型（如参数量达到7B、14B、34B甚至数百B的模型），这会导致显存需求急剧增加。因此，在实际应用中，必须精心设计数据加载策略，例如采用混合精度训练、模型并行化或分布式训练等方式，以优化资源使用，确保在充分利用GPU并行计算能力的同时，不会因为显存不足而影响模型性能。

在深度学习模型中，常见的数值精度包括以下几种：

**双精度浮点数**：在PyTorch中使用`torch.float64`表示，通常用于需要极高精度的应用，但在LLM训练中使用较少。

**全精度浮点数**：在PyTorch中使用`torch.float32`表示，是深度学习中最常用的浮点数精度。

**低精度浮点数**：包括`torch.bfloat16`和`torch.float16`，这两种浮点数各有特点：

1.  **bfloat16**：具有较长的整数部分和较短的小数部分，适合用于减少梯度爆炸的风险（即梯度值过大），但这种精度类型仅在NVIDIA的Ampere系列显卡及以上支持。
2.  **float16**：小数部分较长，能够提供更好的精度控制，但由于整数部分较短，容易发生梯度爆炸。

为了降低显存占用并提高计算速度，可以考虑将浮点数转换为定点数（整数），因为整数计算比浮点数计算更为高效且显存占用更小。在计算精度损失可以接受的情况下，采用定点数进行计算是一种理想的选择，这正是**量化**的基本思路。

量化的核心原理是将每个张量的浮点型数值映射到一个固定范围内的整数。具体来说，原始权重通过一个缩放因子映射到整数范围，然后通过舍入操作丢弃小数部分，从而实现浮点数到定点数的转换。在量化和反量化的过程中，由于舍入操作丢失了小数部分的信息，因此很难完全恢复到原始精度。这种由于量化和反量化造成的误差被称为**精度损失**。简言之，量化过程中的取整操作会导致原始数据的细节丢失，进而在恢复时无法完全恢复原始值。

## 量化分类

根据**量化发生的时机**区分，量化可以分为**PTQ（训练后量化，或离线量化）**和**QAT（训练感知型量化，或在线量化）**。

1.   训练后量化(PTQ)：在模型训练完成后，对已训练好的模型进行量化的过程。这种方法通常不需要访问训练数据，只需模型的权重和一些校准数据来确定量化参数。

     -   优势：不需要重新训练模型，实施起来相对简单快捷；可以直接应用于现有的预训练模型，无需额外的数据集。
     -   缺陷：精度损失可能较大；对于某些复杂的模型，仅靠校准数据可能不足以准确估计量化参数，从而导致性能下降。

     PTQ可以根据是否使用数据集进行校准分为两种类型：**无数据校准(data-free)**和**有数据校准(calibration)**。

     无数据校准(data-free)方法不依赖于外部数据集，而是通过模型本身的权重或其他先验知识来确定量化参数，如缩放因子。这种方法的优点是实现简单，不需要额外的数据集。

     有数据校准(calibration)方法则使用一小部分真实数据来对量化参数进行统计分析和校准，以提高量化后的模型精度。通过真实数据的统计分析，可以更准确地确定量化参数，从而减少量化引入的精度损失，通常能够获得更好的模型性能，但是耗费的时间更长。

2.   训练感知型量化(QAT)：在模型训练阶段就引入量化，模拟量化后的效果来训练模型。这种方式允许模型在训练过程中逐步适应量化带来的变化，从而减少量化后的精度损失。

     -   优势：在训练期间模拟量化，可以使模型更好地适应量化后的环境，通常能获得更高的精度；适合那些对精度要求较高的应用，能够更好地保持模型的性能。
     -   缺陷：需要重新训练模型，增加了训练时间和计算成本；实施起来更为复杂，需要对训练流程进行调整，以支持量化感知训练。

按照**量化方法的不同**，可以将其划分为**线性量化**、**非线性量化（如对数量化）**等多种方式。

1. 线性量化

    线性量化是一种简单且高效的量化方法，它通过一个线性变换将浮点数映射到整数。线性量化又可以根据对称性进一步划分为对称量化和非对称量化。

    - 对称量化

        对称量化是一种特殊的线性量化，它假设权重的分布是对称的，即权重的范围是关于零对称的。这意味着量化操作的零点固定不变。

    - 非对称量化

        非对称量化则是为了解决权重分布不均匀的问题而提出的。非对称量化允许权重的范围不是关于零对称的，因此可以更灵活地适应不同的权重分布。量化操作可以有一个任意的零点，这个零点被映射到量化范围内的某个整数值上。

2. 非线性量化

    通过非线性的变换将浮点数映射到整数。非线性量化的主要目的是更好地适应权重或激活值的分布特性，从而在量化过程中减少精度损失。常见的非线性量化方法包括对数量化、指数量化等。

## 常用量化技术

### BnB量化

BnB全称是BitsAndBytes，是几乎最早集成到transformers框架中的量化算法。

BnB是一种data-free的量化库。该量化方法速度较快，因此可以在模型加载时动态量化，且该方法训练速度较快，因此训练兼容性较好，一般用于QLoRA训练中，且训练后可以合并adapter。当由于其没有数据校准过程，因此精度较AutoGPTQ较低。

BnB采用了针对离群值保持低精度浮点数的做法：

1. 从输入的隐藏状态中，按列提取离群值。
2. 对离群值以低精度浮点型进行矩阵乘法，对非离群值以int8进行矩阵乘法。
3. 对非离群值的结果反量化，将两部分加起来得到最终结果。

### AWQ量化

AWQ全称Activation-aware Weight Quantization，AWQ 提出 LLM 中并非所有权重都同等重要。仅保护 1% 的显著权重可以大大减少量化误差。要识别显著权重通道，我们应该参考激活分布，而不是权重。AWQ 不依赖任何反向传播或重构，因此它可以推广到不同的领域和模态，而不会过度拟合校准集。

AWQ认为：

1. 按照离群值划分不够精确，应当按照“权重的显著性(salient)”来划分。
2. 保持显著性权重为fp16会造成硬件实现的困难和速度的降低，因此应当想办法针对这部分权重进行单独量化。

### GPTQ量化

GPTQ是一种高效的训练后量化（PTQ）方法。GPTQ 的目标是在保持模型精度的前提下，通过量化将模型的权重从浮点数（如 FP16）压缩到低精度整数（如 INT3 或 INT4），从而显著减少模型的存储和计算需求。需要详细了解gptq的原理可以OBD等剪枝算法开始。

GPTQ是基于近似二阶信息进行一次性权重量化。通过以下步骤实现高效且高精度的量化：

1.   任意顺序量化

     *   GPTQ 发现，尽管传统的贪心量化方法（如 OBQ）通过选择当前量化误差最小的权重进行量化可以获得较好的结果，但这种方法在大规模模型上并没有显著优于任意顺序量化。
     *   GPTQ 采用任意固定顺序对权重进行量化，这大大简化了算法的实现，并且在大规模模型上仍然能够保持良好的性能。

2.   延迟批量更新

     -   直接实现逐权重更新的算法在实际应用中效率低下，因为每次更新都需要访问大量的内存。

     -   GPTQ 引入了延迟批量更新（lazy-batch updates）机制，将多个列的更新合并在一起进行批量处理。
     -   尽管该策略并没有减少理论计算量，但它有效地解决了内存吞吐量瓶颈从而显著提高了 GPU 的利用率和计算效率。

3.   Cholesky 分解

     -   在量化过程中，需要频繁计算和更新 Hessian 矩阵的逆。直接计算 Hessian 矩阵的逆在大规模模型上容易出现数值不稳定的问题。
     -   GPTQ 使用 Cholesky 分解预先计算所有必要的 Hessian 矩阵信息，并在量化过程中逐步更新这些信息，从而避免了数值不稳定的问题，提高了算法的鲁棒性和效率。

GPTQ的量化操作基于泰勒级数分解，其评估公式依赖于Hessian矩阵，并和输入X强相关，因此需要迭代更新，速度慢但更准确。

### HQQ量化

HQQ即Half-Quadratic Quantization，是一种称为半二次量化（HQQ）的新量化技术。我们的方法不需要校准数据，显着加快了大型模型的量化速度，同时提供了与基于校准的方法相媲美的压缩质量。

数据校准的方法会有以下问题

1.   校准数据偏差：根据提供的校准数据，量化质量可能会受到负面影响。 

2.   量化时间：校准可能是一个繁重的计算过程，尤其是对于非常大的模型

HQQ方法特别关注最小化权重误差而不是层激活。通过结合稀疏性促进损失通过超拉普拉斯分布有效地对异常值进行建模。HQQ量化的问题定义在如何在零点量化中取得最优的z和s（零点和缩放倍数）该方法和输入无关，因此不需要量化集。

### 总结

*   **BnB 量化**：通过设置阈值区分离群点和非离群点，离群点保持高精度，非离群点进行 8 位量化。4 位量化时使用 nf4 格式，数值分布为正态分布，更好地适应 LLM 的参数分布。
*   **AWQ**：通过评估激活值尺度确定重要参数，并对这些参数按组进行缩放，减少重要参数的量化损失，提高模型精度，同样需要量化集辅助量化。
*   **GPTQ**：使用泰勒级数分解和海森矩阵评估参数重要性，通过并行计算减少显存占用和提高处理速度，但需要量化集辅助量化。
*   **HQQ**：通过对零点量化的公式进行转换，将其分解为两个子问题分别求解，找到最优的零点 z，对输入数据无特殊要求，不需要量化集辅助量化。

## LLM量化

Swift 支持通过 AWQ、GPTQ、BNB、HQQ、EETQ 等技术对模型进行量化。这些量化方法各有特点，选择合适的量化方法可以优化模型的性能和推理速度。具体来说：

- **AWQ** 和 **GPTQ** 量化方法需要使用校准数据集，量化性能较好，但量化过程较慢，且支持 vLLM 推理加速。
- **BNB**、**HQQ** 和 **EETQ** 量化方法不需要校准数据，量化速度较快，适用于快速部署。

这五种量化方法都支持通过 QLoRA 微调来进一步优化模型。

- **AWQ** 和 **GPTQ** 需要使用 `swift export` 命令进行量化
- **BNB**、**HQQ** 和 **EETQ** 则可以在 `sft` 和 `infer` 阶段进行快速量化。

> **提示**：如果在进行 AWQ 或 GPTQ 量化时遇到报错，建议尝试安装 `vllm`（具体内容请参考案例九）。

1. 根据选择的量化方法，安装相应的依赖库：

   ```shell
   # 使用awq量化:
   pip install autoawq -U
   
   # 使用gptq量化:
   pip install auto_gptq optimum -U
   
   # 使用bnb量化：
   pip install bitsandbytes -U
   
   # 使用hqq量化：
   pip install hqq
   ```

2. 下载量化训练数据集

   进入 `LLM/data` 目录，并创建 `quantization` 文件夹以存放量化数据集。

   ```shell
   # 进入 data 目录
   cd ~/data
   
   # 创建quantization目录
   mkdir quantization
   #进入quantization目录
   cd quantization
   
   # 下载数据集alpaca_zh
   git clone https://www.modelscope.cn/datasets/llamafactory/alpaca_zh.git
   ```

3. 使用`AWQ`量化

   对于 `AWQ` 量化，我们使用 `transformers` 库来进行量化。以下是相关步骤：

   ```shell
   #返回脚本文件夹
   cd ..
   cd script
   
   # 创建并进入quantizationScript目录
   mkdir quantizationScript
   cd quantizationScript
   
   # 创建autoAWQ.py量化处理程序
   vim autoAWQ.py
   
   # 运行程序
   CUDA_VISIBLE_DEVICES=0 python src/autoAWQ.py
   ```

   `autoAWQ.py` 文件的具体内容如下：

   ```python
   import json
   from awq import AutoAWQForCausalLM
   from transformers import AutoTokenizer
   from datasets import load_dataset
   
   def qwen_preprocess(lora_data_, tokenizer_, max_len_):
       messages = []
       for item in lora_data_:
           temp = [
               {"role": "system", "content": item['instruction']},
               {"role": "user", "content": item['input']},
               {"role": "assistant", "content": item['output']}
           ]
           messages.append(temp)
       data = []
       for msg in messages:
           text = tokenizer_.apply_chat_template(msg, tokenize=False, add_generation_prompt=False)
           data.append(text)
       return data
   # 定义原始模型路径
   model_path = 'model/Qwen2.5-1.5B-Instruct'
   # model_path = 'output/sft_full/qwen2_5-1_5b-instruct/v3-20241101-225043/checkpoint-13930'
   # 量化后模型的输出路径
   quant_path = 'output/model/Qwen2.5-1.5B-Instruct-awq-int4-2'
   # quant_path = 'output/sft_full/qwen2_5-1_5b-instruct/v3-20241101-225043/checkpoint-13930-awq-int4-2'
   print(quant_path)
   # 量化的配置
   quant_config = { "zero_point": True, "q_group_size": 128, "w_bit": 4, "version": "GEMM" }
   
   # 加载模型
   model = AutoAWQForCausalLM.from_pretrained(
       model_path, **{"low_cpu_mem_usage": True, "use_cache": False}
   )
   # 加载分词器
   tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
   # 加载量化数据集
   quantize_dataset_path = 'data/quantization/alpaca_zh/alpaca_data_zh_51k.json'
   with open(quantize_dataset_path, 'r', encoding='utf-8') as f:
       lora_data = json.load(f)
   max_len = 1024
   data = qwen_preprocess(lora_data, tokenizer, max_len)
   
   # 对模型进行量化
   model.quantize(tokenizer, quant_config=quant_config, calib_data=data)
   
   # 保存量化后的模型
   model.save_quantized(quant_path)
   tokenizer.save_pretrained(quant_path)
   
   print(f'Model is quantized and saved at "{quant_path}"')
   
   ```

   如果量化过程顺利完成，将显示如下信息：

   ```shell
   Model is quantized and saved at "output/qwen2_5-1_5b-instruct-awq-int4"
   ```

   接下来，使用swift对量化模型进行部署并测试性能，部署命令如下：

   ```shell
   CUDA_VISIBLE_DEVICES=0 swift infer --stream true --model_type qwen2_5 --model /home/qluai/WEM/yan1/project1/output/quantization/Qwen2.5-1.5B-Instruct-awq-int4
   ```

   示例输出（未量化和量化后的对比）：

   ```shell
   没有经过量化版本：
   问：我肚子不是很舒服怎么办？
   答：如果您感到不适，可以尝试以下方法：\n\n1. 休息：找一个安静的地方躺下或坐下，放松身体。\n\n2. 喝水：喝一些温水可以帮助缓解胃部不适。\n\n3. 饮食调整：避免食用油腻、辛辣、刺激性食物，多吃易消化的食物如米粥、面条等。\n\n4. 按摩：轻轻按摩腹部有助于促进肠胃蠕动，缓解腹胀和疼痛。\n\n5. 药物治疗：如果症状严重 ，可以考虑服用一些药物，但最好先咨询医生的意见。\n\n如果您的症状持续时间较长或者伴随其他症状（如呕吐、腹泻等），建议及时就医。
   
   量化后：
   问：我肚子不是很舒服怎么办？
   答：如果您感到肚子不舒服，可以尝试以下方法：\n\n1. 休息：找一个安静的地方躺下或坐下，放松身体。\n\n2. 喝水：喝一些温水可以帮助缓解胃部不适。\n\n3. 饮食调整：避免食用油腻、辛辣、刺激性食物和饮料。可以选择清淡易消化的食物，如米粥、面条等。\n\n4. 药物治疗：如果症状严重，可以考虑服用一些药物，如抗酸药、止泻药等。\n\n5. 就医：如果症状持续时间较长或者伴随其他症状（如呕吐、腹泻、发热等），建议及时就医。\n\n请注意，以上方法仅供参考，具体应根据个人情况选择适合自己的方法。如果症状加重或出现其他异常，请尽快就医。
   ```

   接下来，我们可以使用 `swift` 命令进行量化部署。

   ```shell
   # AWQ 基座模型量化
   CUDA_VISIBLE_DEVICES=0 swift export \
       --quant_bits 4 \
       --model /home/qluai/WEM/yan1/project1/models/Qwen2.5-1.5B-Instruct \
       --dataset /home/qluai/WEM/yan1/project1/data/quantization/alpaca_zh/alpaca_data_zh_51k.json \
       --quant_method awq
   	--merge_lora true # 如果是LoRA微调模型，需要添加此参数
   
       
   # 部署脚本
   # AWQ 基座模型量化部署
   CUDA_VISIBLE_DEVICES=0 swift infer --stream true --model_type qwen2_5 --model /home/qluai/WEM/yan1/project1/output/quantization/awq/Qwen2.5-1.5B-Instruct-awq-int4
   
   # AWQ 微调模型量化部署
   CUDA_VISIBLE_DEVICES=0 swift infer --stream true --model_type qwen2_5 --ckpt_dir output/sft_full/qwen2_5-1_5b-instruct/v3-20241101-225043/checkpoint-13930-awq-int4
   
   ```
   
4. 使用`GPTQ`量化

   对于`GPTQ` 量化，我们同样使用 `swift export` 命令进行量化。以下是相关命令：

   ```shell
   # GPTQ 基座模型量化
   CUDA_VISIBLE_DEVICES=0 swift export \
       --model /home/qluai/WEM/yan1/project1/models/Qwen2.5-1.5B-Instruct \
       --quant_bits 4 \
       --dataset /home/qluai/WEM/yan1/project1/data/quantization/alpaca_zh/alpaca_data_zh_51k.json \
       --quant_method gptq
   
   
   # GPTQ 微调模型量化
   CUDA_VISIBLE_DEVICES=0 swift export \
       --quant_bits 4 \
       --dataset /home/qluai/WEM/yan1/project1/data/quantization/alpaca_zh/alpaca_data_zh_51k.json  \
       --quant_method gptq \
       --ckpt_dir ./output/sft_full/qwen2_5-1_5b-instruct/v3-20241101-225043/checkpoint-13930
   
   ```

   量化后出现下述结果即量化成功：

   ```shell
   [INFO:swift] Saving quantized weights...
   [INFO:swift] Successfully quantized the model and saved in 
   [INFO:swift] End time of running main: 2024-11-23 16:09:35.720813
   
   ```

   部署查看效果：

   ```shell
   # GPTQ 基座模型量化部署
   CUDA_VISIBLE_DEVICES=0 swift infer --stream true --model  /home/qluai/WEM/yan1/project1/output/quantization/GPTQ/Qwen2.5-1.5B-Instruct-gptq-int4
   # GPTQ 微调模型量化部署
   CUDA_VISIBLE_DEVICES=0 swift deploy --model_type qwen2_5-1_5b-instruct --ckpt_dir  output/sft_full/qwen2_5-1_5b-instruct/v3-20241101-225043/checkpoint-13930-gptq-int4
   
   ```

   结果展示：

   ```shell
   没有经过量化版本：
   问：我肚子不是很舒服怎么办？
   答：如果您感到不适，可以尝试以下方法：\n\n1. 休息：找一个安静的地方躺下或坐下，放松身体。\n\n2. 喝水：喝一些温水可以帮助缓解胃部不适。\n\n3. 饮食调整：避免食用油腻、辛辣、刺激性食物，多吃易消化的食物如米粥、面条等。\n\n4. 按摩：轻轻按摩腹部有助于促进肠胃蠕动，缓解腹胀和疼痛。\n\n5. 药物治疗：如果症状严重 ，可以考虑服用一些药物，但最好先咨询医生的意见。\n\n如果您的症状持续时间较长或者伴随其他症状（如呕吐、腹泻等），建议及时就医。
   
   量化后：
   <<< 我肚子不是很舒服怎么办？
   如果您的肚子不舒服，可能是因为您正在经历消化不良、胃酸反流或者是肠胃疾病等。建议您先休息，并尝试喝些温水来缓解不适感。同时，避免食用油腻、辛辣或者难以消化的食物，保持饮食清淡，适量运动也有助于改善症状。
   
   但如果您的情况持续不改或伴有其他严重症状（如剧烈疼痛、呕吐、血便等），请尽快就医，寻求专业的医疗帮助。自我诊断和治疗可能会延误病情，导致更严重的健康问题。
   ```

5. 使用`BNB`量化

   对于`BNB`，我们需要使用`swift infer`或`swift deploy`来进行快速量化并推理。

   对于基座模型使用`swift infer`：

   ```shell
   # bnb 基座模型量化
   CUDA_VISIBLE_DEVICES=0 swift infer \
       --model /home/qluai/WEM/yan1/project1/models/Qwen2.5-1.5B-Instruct \
       --quant_method bnb \
       --quant_bits 4
   ```

   对于微调后的模型使用`swift deploy`：

   ```shell
   # bnb 微调模型量化
   CUDA_VISIBLE_DEVICES=0 swift deploy \
       --quant_method bnb \
       --quant_bits 4 \
       --ckpt_dir ./output/sft_full/qwen2_5-1_5b-instruct/v3-20241101-225043/checkpoint-13930
   
   ```

   两个模型的结果如下所示：

   ```shell
   没有经过量化版本：
   问：我肚子不是很舒服怎么办？
   答：如果您感到不适，可以尝试以下方法：\n\n1. 休息：找一个安静的地方躺下或坐下，放松身体。\n\n2. 喝水：喝一些温水可以帮助缓解胃部不适。\n\n3. 饮食调整：避免食用油腻、辛辣、刺激性食物，多吃易消化的食物如米粥、面条等。\n\n4. 按摩：轻轻按摩腹部有助于促进肠胃蠕动，缓解腹胀和疼痛。\n\n5. 药物治疗：如果症状严重 ，可以考虑服用一些药物，但最好先咨询医生的意见。\n\n如果您的症状持续时间较长或者伴随其他症状（如呕吐、腹泻等），建议及时就医。
   
   量化后：
   <<< 我肚子不是很舒服怎么办？
   如果您的肚子不舒服，您可以尝试以下方法：
   
   1. 尝试深呼吸或进行放松练习。
   2. 休息一下，不要做剧烈的运动。
   3. 喝一杯温水或者喝一些热水。
   4. 转动身体，轻轻地活动四肢。
   5. 如果症状持续不减，请及时就医。
   
   如果您需要更多的帮助或建议，请随时向我寻求。
   ```

6. 使用`HQQ`量化

   对于`HQQ`我们同样使用`swift infer`或`swift deploy`来进行快速量化并推理。

   对于基座模型：

   ```shell
   # hqq 基座模型量化
   CUDA_VISIBLE_DEVICES=0 swift infer \
       --model /home/qluai/WEM/yan1/project1/models/Qwen2.5-1.5B-Instruct \
       --quant_method hqq \
       --quant_bits 4
   ```
   

对于微调后的模型：

```shell
   # hqq 微调模型量化
   CUDA_VISIBLE_DEVICES=0 swift deploy \
       --quant_method hqq \
       --quantization_bit 4 \
       --ckpt_dir ./output/sft_full/qwen2_5-1_5b-instruct/v3-20241101-225043/checkpoint-13930
```

结果展示：

```shell
   没有经过量化版本：
   问：我肚子不是很舒服怎么办？
   答：如果您感到不适，可以尝试以下方法：\n\n1. 休息：找一个安静的地方躺下或坐下，放松身体。\n\n2. 喝水：喝一些温水可以帮助缓解胃部不适。\n\n3. 饮食调整：避免食用油腻、辛辣、刺激性食物，多吃易消化的食物如米粥、面条等。\n\n4. 按摩：轻轻按摩腹部有助于促进肠胃蠕动，缓解腹胀和疼痛。\n\n5. 药物治疗：如果症状严重 ，可以考虑服用一些药物，但最好先咨询医生的意见。\n\n如果您的症状持续时间较长或者伴随其他症状（如呕吐、腹泻等），建议及时就医。
   
   量化后：
   <<< 我肚子不是很舒服怎么办？
   当您感到不适时，可以采取以下措施：
   
   1. **休息**：找一个安静的地方坐下或躺下，尽量放松身体。
2. **深呼吸**：尝试进行几次深呼吸，帮助减轻紧张和焦虑感。
   3. **温和运动**：如散步等轻度运动，有助于缓解肌肉紧张和促进血液循环。
4. **喝水**：确保充分补水，避免脱水引起的不适。
   5. **调整饮食**：暂时避免油腻、辛辣食物，可以选择清淡易消化的食物。
   6. **咨询医生**：如果症状持续不减或者加重，请及时就医。
   
   请根据自己的实际情况选择合适的方法，并在必要时寻求专业医疗建议。希望这些建议能帮到您！
```

   `swift export`的参数解析

   *   `--to_peft_format`: 默认为`False`. 将lora的swift format（`--tuner_backend swift`）转成peft format.
   *   `--merge_lora`: 默认为`False`. 该参数已在InferArguments中定义, 不属于新增参数. 是否将lora权重merge到基模型中, 并保存完整的权重. 权重会保存在`ckpt_dir`的同级目录中, e.g. `'/path/to/your/vx-xxx/checkpoint-xxx-merged'`目录下.
   *   `--quant_bits`: 量化的bits数. 默认为`0`, 即不进行量化. 如果你设置了`--quant_method awq`, 你可以设置为`4`进行4bits量化. 如果你设置了`--quant_method gptq`, 你可以设置为`2`,`3`,`4`,`8`进行对应bits的量化. 如果对原始模型进行量化, 权重会保存在`f'{args.model_type}-{args.quant_method}-int{args.quant_bits}'`目录中. 如果对微调后模型进行量化, 权重会保存在`ckpt_dir`的同级目录中, e.g. `f'/path/to/your/vx-xxx/checkpoint-xxx-{args.quant_method}-int{args.quant_bits}'`目录下.
   *   `--quant_method`: 量化方法, 默认为`'awq'`. 你可以选择为'awq', 'gptq', 'bnb'.
   *   `--dataset`: 该参数已在InferArguments中定义, 在export时含义为量化数据集. 默认为`[]`. 
   *   `--quant_n_samples`: 量化参数, 默认为`256`. 当设置为`--quant_method awq`时, 如果出现量化的时候OOM, 可以适度降低`--quant_n_samples`和`--quant_seqlen`. `--quant_method gptq`通常不会出现量化OOM.
   *   `--quant_seqlen`: 量化参数, 默认为`2048`.
   *   `--quant_batch_size`: 量化数据集的batch_size，默认为`1`.
   *   `--quant_device_map`: 默认为`None`. 你可以指定为'cuda:0', 'auto', 'cpu'等, 表示量化时模型导入的设备.
   *   `--quant_output_dir`: 默认为`None`, 默认的quant_output_dir会被打印在命令行中.
   *   `--push_to_hub`: 默认为`False`. 是否将最后的`ckpt_dir`push到ModelScope Hub中. 如果你指定了`merge_lora`, 则将推送全量参数; 如果你还指定了`quant_bits`, 则将推送量化后的模型.
   *   `--hub_model_id`: 默认为`None`. 推送到的ModelScope Hub的model_id. 如果`push_to_hub`设置为True, 该参数必须被设置.
   *   `--hub_token`: 默认为`None`. 具体的参数介绍可以在`sft命令行参数`中查看.
   *   `--hub_private_repo`: 默认为`False`. 具体的参数介绍可以在`sft命令行参数`中查看.
   *   `--commit_message`: 默认是`'update files'`.
   *   `--to_ollama`: 转为ollama导出.
   *   `--ollama_output_dir`: ollama输出目录. 默认存储在当前目录下的`模型类型-ollama`文件夹内.

---

![](QLUNLP_logo.png)