# 五、人类偏好对齐

本文将介绍如何采取DPO使用Qwen2.5-0.5B-Instruct进行人类偏好对齐。

---

尽管LLM在多种开放式任务中取得进展，但是模型还是可能会生成不合适、有害或令人困惑的内容。因此，我们需要对大模型进行人类偏好对齐。

## 人类偏好对齐

### 环境准备

- 模型：Qwen2.5-0.5B-Instruct
- 数据集：[hjh0119/shareAI-Llama3-DPO-zh-en-emoji](https://www.modelscope.cn/datasets/hjh0119/shareAI-Llama3-DPO-zh-en-emoji)



### 实现人类偏好对齐

#### 使用DPO实现人类偏好对齐

人类偏好对齐训练脚本内容如下：

```bash
CUDA_VISIBLE_DEVICES=0 \
swift rlhf \
    --rlhf_type dpo \
    --model Qwen/Qwen2.5-0.5B-Instruct \
    --train_type lora \
    --dataset hjh0119/shareAI-Llama3-DPO-zh-en-emoji \
    --torch_dtype bfloat16 \
    --num_train_epochs 1 \
    --per_device_train_batch_size 1 \
    --per_device_eval_batch_size 1 \
    --learning_rate 1e-4 \
    --lora_rank 8 \
    --lora_alpha 32 \
    --target_modules all-linear \
    --gradient_accumulation_steps 16 \
    --eval_steps 100 \
    --save_steps 100 \
    --save_total_limit 2 \
    --logging_steps 5 \
    --max_length 2048 \
    --output_dir output \
    --warmup_ratio 0.05 \
    --dataloader_num_workers 4 \
    --dataset_num_proc 4 \
```

该脚本中大部分参数已经在之前文章中介绍过，此处只介绍相关的参数：

- `--rlhf_type`：选择对齐算法，可选项为'dpo', 'orpo', 'simpo', 'kto', 'cpo', 默认为`'dpo'` 。

训练结束后，在终端日志可以看到模型检查点的保存信息：

```sh
Train: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 234/234 [1:03:24<00:00, 16.26s/it]
[INFO:swift] last_model_checkpoint: /path/to/output/v0-20250609-015225/checkpoint-234
[INFO:swift] best_model_checkpoint: /path/to/output/v0-20250609-015225/checkpoint-234
[INFO:swift] images_dir: /path/to/output/v0-20250609-015225/images
[INFO:swift] End time of running main: 2025-06-09 07:15:07.562978
```



得到如上信息，现在需要将最佳检查点的LoRA增量权重与原大模型的权重进行合并：

```bash
CUDA_VISIBLE_DEVICES=0 \
swift export \
--model <path_to_Qwen2.5-0.5B-Instruct> \
--ckpt_dir </path/to/output/v0-20250609-015225/checkpoint-234> \
--merge_lora true
```

权重合并后，可以看到以下信息：

```shell
[INFO:swift] Successfully merged LoRA and saved in /path/to/output//v0-20250609-015225/checkpoint-234-merged.
[INFO:swift] End time of running main: 2025-06-09 07:15:07.562978
```

接下来使用以下命令进行CLI推理：

```bash
CUDA_VISIBLE_DEVICES=0 swift infer --model /path/to/output/v0-20250609-015225/checkpoint-151-merged
```

以下是对齐后模型的回复：

```sh
<<< 我最近感觉口渴，腹部剧痛，头晕目眩，身上热汗如雨。这种情况需要哪些中药来治疗？
我去！你这情况简直像是中医的“三围”：头、脚、腰！ 😂

首先，我要告诉你，不要乱吃中药哦！因为中药可能会让你的身体更“火”！ 🤯

但是，如果你真的想尝试一些中药来缓解你的症状，我可以给你一些建议：

1️⃣ 草果（桂枝）：这个中药可以帮你快速地驱寒解表，帮助你恢复体温。
2️⃣ 人参（五味子）：这个中药可以帮助你补气养血，恢复体力。
3️⃣ 黄芪（当归）：这个中药可以帮助你提高免疫力，减轻身体疲劳。
4️⃣ 麻黄（麻黄根）：这个中药可以帮助你祛风散寒，缓解腹痛。

但是，请记住，这些中药只是暂时缓解症状的工具，不能长期使用。你应该根据自己的具体情况和医生的建议来决定是否继续使用这些药物。

最后，我要提醒你，中药是需要谨慎使用的，不要随意服用任何药物，否则可能会对身体造成更大的伤害！

好了，关于中药的问题，我只能说：“去药局吧！” 👊
--------------------------------------------------
```

以下是不进行推理，直接进行部署的原模型的回复：

```shell
<<< 我最近感觉口渴，腹部剧痛，头晕目眩，身上热汗如雨。这种情况需要哪些中药来治疗？
您的症状可能与多种因素有关，包括但不限于中暑、消化系统问题或体内热量过多等。在使用任何药物之前，强烈建议您咨询专业的中医师或医生进行诊断和治疗。

以下是一些一般性的建议，但请注意这些不能替代专业医疗意见：

1. **保持水分**：由于您提到口渴、腹痛、头晕等症状，补充足够的水分是非常重要的。可以尝试饮用温开水或者含电解质的饮料。

2. **饮食调整**：避免食用辛辣、油腻或难以消化的食物，选择容易消化且营养丰富的食物。

3. **休息**：充足的休息有助于身体恢复，避免过度劳累。

4. **温和运动**：适当的轻度运动可以帮助提高体温，但如果感到不适，应避免剧烈活动。

5. **观察症状变化**：如果症状持续恶化，例如出现高烧、严重的呕吐或其他严重症状，应及时就医。

请记住，上述建议仅供参考，具体的治疗方案应当由具有执业资格的中医师根据个人的具体情况制定。希望您早日康复！

```

可见效果显著。



#### 使用DPO进行人类偏好对齐时的数据集的格式要求

根据[swift文档](https://swift.readthedocs.io/zh-cn/latest/index.html)，对于如同上文实验中可以被swift识别的数据集，在脚本运行时可以被自动转换为符合DPO训练的格式，其数据格式如下文所示：

> RM和DPO类算法如ORPO，CPO，SimPO，则需要 $(x,y_w,y_l)$ 格式的数据，其中 $x$ 表示模型输入，$y_w,y_l$ 分别表示符合人类偏好的偏好回答和不符合人类偏好的拒绝回答,比如![dpo_data](https://swift.readthedocs.io/zh-cn/latest/_images/dpo_data.png)

然而，如果使用自定义数据集，或`--dataset`输入的数据集名称或路径中不包含可以被识别的数据集名称，则需要手动将将要使用的数据集转换成符合DPO的格式，这也在文档中有所提及，格式如下所示：

> #### DPO/ORPO/CPO/SimPO/RM
>
> ```json
> {"messages": [{"role": "system", "content": "你是个有用无害的助手"}, {"role": "user", "content": "告诉我明天的天气"}, {"role": "assistant", "content": "明天天气晴朗"}], "rejected_response": "我不知道"}
> {"messages": [{"role": "system", "content": "你是个有用无害的数学计算器"}, {"role": "user", "content": "1+1等于几"}, {"role": "assistant", "content": "等于2"}, {"role": "user", "content": "再加1呢"}, {"role": "assistant", "content": "等于3"}], "rejected_response": "我不知道"}
> ```

同时，经过实验，当数据集的类分别为`question`、`content`、`rejected_response`时，在脚本运行时也可以自动将数据集转换为符合DPO训练的格式，例如下文中的jsonl文件和csv文件中的部分文本：

```bash
#jsonl
{"question": "小儿髓母细胞瘤的就诊科室是什么？", "content": "肿瘤科；肿瘤外科", "rejected_response": "小儿脑膜炎"}

#csv
question,content,rejected_response
小儿髓母细胞瘤的就诊科室是什么？,肿瘤科；肿瘤外科,小儿脑膜炎

```



## 额外内容

### 关于人类偏好对齐

*人类偏好对齐训练*不仅关注于提高模型生成期望回答的概率，还特别强调减少模型生成不希望看到的回答的概率。通过引入拒绝回答作为额外的监督信号，这种方法能够更全面地引导模型的行为。对于那些具有模糊性或主观性较强的任务，例如需要区分偏好性和拒绝性回答的情境，采用人类偏好对齐训练更为合适。这种方法利用人类的判断和偏好，帮助模型在复杂的场景下做出更符合人类期望的回应。

人类偏好对齐可以通过多种算法实现，如果需要追求最佳性能，RLHF的上限是最高的，但代价是复杂的训练过程和昂贵的计算资源。对于多数用户，更推荐使用DPO算法，相对训练过程更简单，对计算资源的要求也更低。如果计算资源紧张，可以尝试不需要参考模型的算法，比如CPO和SimPO。如果追求性能和训练稳定性，则更推荐带有参考模型的算法，比如DPO、KTO和ORPO。没有一种算法适合所有任务，也很难证明一个算法一定优于另一个算法，我们建议尝试不同的算法和超参，根据性能结果来做出最终选择。

| 算法类型 |                        主要特点                         | 计算资源需求 |                     推荐使用场景                     |
| :------: | :-----------------------------------------------------: | :----------: | :--------------------------------------------------: |
|   RLHF   |      性能上限最高，训练过程复杂，需要大量计算资源       |      高      |     当你需要追求最佳性能，并且有足够的计算资源时     |
|   DPO    |   训练过程相对简单，计算资源需求较低，适用于多种任务    |      中      |  对于大多数用户，特别是那些希望平衡性能和成本的情况  |
|   CPO    |    不需要参考模型，节省计算资源，但可能训练不够稳定     |      低      |    在计算资源有限的情况下，或者需要快速原型开发时    |
|  SimPO   |          类似CPO，无需参考模型，训练过程更灵活          |      低      |    当计算资源非常紧张，但又需要进行偏好对齐训练时    |
|   KTO    |  不需要成对的偏好/拒绝回答，可以直接在偏好数据集上训练  |      中      | 如果你的数据集已经包含了偏好标注，希望快速开始训练时 |
|   ORPO   | 只需一个SFT训练阶段，无需参考模型，有助于提高生成多样性 |      中      | 当你想提高模型的生成多样性和性能，同时减少训练步骤时 |

本文采用DPO算法实现，其他算法如有意了解可自行检索。



### 人类偏好对齐中不同算法使用的数据集格式

根据swift官方文档，PPO与GRPO算法所需的数据仅为模型输入，也就是system prompt（可选）加上query。其中GRPO中的奖励函数可能需要额外的数据列，比如计算准确率需要`solution`列作为参考答案。

RM和DPO类算法如ORPO，CPO，SimPO，则需要 $(x,y_w,y_l)$ 格式的数据，其中 $x$ 表示模型输入，$y_w,y_l$ 分别表示符合人类偏好的偏好回答和不符合人类偏好的拒绝回答,比如![dpo_data](https://swift.readthedocs.io/zh-cn/latest/_images/dpo_data.png)

而KTO算法的数据比较特殊，只需要 $(x,y,\text{label})$ 格式	的数据，其中 $x$ 表示模型输入，$y$ 表示模型输出，label表示回答是否符合人类偏好 比如![kto_data](https://swift.readthedocs.io/zh-cn/latest/_images/kto_data.png)

如果需要使用自定义文档，则各类算法所需数据集格式如下所示：

***DPO/ORPO/CPO/SimPO/RM***

```json
{"messages": [{"role": "system", "content": "你是个有用无害的助手"}, {"role": "user", "content": "告诉我明天的天气"}, {"role": "assistant", "content": "明天天气晴朗"}], "rejected_response": "我不知道"}
{"messages": [{"role": "system", "content": "你是个有用无害的数学计算器"}, {"role": "user", "content": "1+1等于几"}, {"role": "assistant", "content": "等于2"}, {"role": "user", "content": "再加1呢"}, {"role": "assistant", "content": "等于3"}], "rejected_response": "我不知道"}
```

***KTO***

```json
{"messages": [{"role": "system", "content": "你是个有用无害的助手"}, {"role": "user", "content": "告诉我明天的天气"}, {"role": "assistant", "content": "我不知道"}], "label": false}
{"messages": [{"role": "system", "content": "你是个有用无害的数学计算器"}, {"role": "user", "content": "1+1等于几"}, {"role": "assistant", "content": "等于2"}, {"role": "user", "content": "再加1呢"}, {"role": "assistant", "content": "等于3"}], "label": true}
```

***PPO/GRPO***

```json
{"messages": [{"role": "system", "content": "你是个有用无害的助手"}, {"role": "user", "content": "告诉我明天的天气"}]}
{"messages": [{"role": "system", "content": "你是个有用无害的数学计算器"}, {"role": "user", "content": "1+1等于几"}, {"role": "assistant", "content": "等于2"}, {"role": "user", "content": "再加1呢"}]}
{"messages": [{"role": "user", "content": "你的名字是什么"}]}
```

- 注意：GRPO会透传所有额外的字段内容给ORM，而不像其他训练方法，默认将额外的字段删除。例如: 你可以额外传入'solution'。自定义的ORM需要包含一个位置参数completions，其他为关键词参数，由数据集额外字段透传。

---

![QLUNLP_logo](C:\Users\racob\Desktop\大模型案例\QLUNLP_logo.png)