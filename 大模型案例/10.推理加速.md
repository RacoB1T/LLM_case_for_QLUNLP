# 十、推理加速

---

vLLM 是一个高效、易用的 LLM 推理和服务库，专为大规模推理优化，旨在提供出色的性能和高吞吐量。通过创新的页式注意力机制，vLLM 高效管理了注意力键值内存，使得它能够连续处理多个请求并保持较高的执行效率。此外，vLLM 采用了 CUDA 来加速推理过程，进一步提升了执行速度。

vLLM 支持多种量化技术，包括 GPTQ、AWQ、INT4、INT8 和 FP8，这些技术使得它能够在不牺牲精度的情况下，减少计算和内存的需求。vLLM 还与 FlashAttention 和 FlashInfer 进行了深度集成，进一步提升了推理效率。为了进一步提升推理性能，其还采用了投机解码和分块预填充等先进技术，这些技术能有效减少延迟，提高响应速度。

vLLM 的灵活性和易用性也是一大亮点。它可以无缝集成到流行的 Hugging Face 模型中，支持高吞吐量服务，提供并行采样、束搜索等多种解码算法。该库还支持张量并行性和管道并行性，适用于分布式推理场景，同时支持流式输出和兼容 OpenAI 的 API 服务器。

## 9.1 swift运行vLLM

首先，我们需要安装 `vLLM` 库：

```bash
pip install vllm
```

环境参考：

```
cuda 12.4
swfit 2.35.0
torch 2.4
vllm 0.6.2
transformers 4.52.4
tokenizers 0.21.1
```

### 9.1.1 通过`python`程序使用vLLM进行推理

通过以下步骤，可以使用 Python 程序实现对 LLM 的推理：

```shell
# 进入LLM目录
cd ~/LLM

# 创建vllm_swift.py程序运行LLM
vim src/vllm_swift.py

python src/vllm_swift.py
```

`vllm_swift.py` 程序代码如下：

```python
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer

# 加载Qwen专用tokenizer（确保格式对齐）
tokenizer = AutoTokenizer.from_pretrained("/mnt/suke/LLM/model/Qwen2.5-1.5B-Instruct")

# 初始化模型（启用确定性配置）
llm = LLM(
    model="/mnt/suke/LLM/model/Qwen2.5-1.5B-Instruct",
    dtype="half",
    gpu_memory_utilization=0.9,
    enforce_eager=True,
    block_size=32,  # 提升长文本稳定性 [3](@ref)
    disable_log_stats=True
)

# 严格约束的生成参数
sampling_params = SamplingParams(
    temperature=0.3,
    top_p=0.9,
    min_p=0.02,
    repetition_penalty=1.1,
    frequency_penalty=0.2,
    presence_penalty=0.1,
    max_tokens=128,
    stop_token_ids=[tokenizer.eos_token_id]  # 直接使用tokenizer的eos_id
)

# 构建符合Qwen指令格式的提示 [4,8](@ref)
def build_prompt(user_query):
    return [
        {"role": "system", "content": "你是一名医生，回答需基于医学常识"},
        {"role": "user", "content": user_query}
    ]

prompts = [
    tokenizer.apply_chat_template(build_prompt("你好！"), tokenize=False),
    tokenizer.apply_chat_template(build_prompt("我有些肚子疼，怎么办"), tokenize=False)
]

# 预热模型（避免首次生成异常）[3](@ref)
_ = llm.generate("预热", sampling_params=SamplingParams(max_tokens=10))

# 正式推理
outputs = llm.generate(prompts, sampling_params)
for output in outputs:
    print(f"输入：{output.prompt}\n生成：{output.outputs[0].text.strip()}\n")
```

运行结果：

```shell
输入：<|im_start|>system
你是一名医生，回答需基于医学常识<|im_end|>
<|im_start|>user
你好！<|im_end|>

生成：superior
您好！有什么我可以帮助您的吗？

输入：<|im_start|>system
你是一名医生，回答需基于医学常识<|im_end|>
<|im_start|>user
我有些肚子疼，怎么办<|im_end|>

生成：superintendent
如果你感到腹痛，首先应该注意休息，并且尽量避免进食油腻、辛辣或者难以消化的食物。                                                                 可以尝试喝些温水或是热饮料来缓解症状。如果疼痛持续不减或伴有其他严重症状（如高烧、呕吐、血便等），应立即就医。

在等待医疗帮助的过程中，保持平静，不要过度活动，以免加重病情。希望你能尽快感觉好转！

```

### 9.1.2`swift deploy` 部署及调用

借助 vLLM，构建一个兼容 OpenAI API 的 API 服务非常简单。此服务可用作实现 OpenAI API 协议的服务器进行部署。默认情况下，服务器将在 `http://localhost:8000` 启动。可以使用 `--host` 和 `--port` 参数来指定自定义地址。

```shell
# 部署基座模型代码如下：
CUDA_VISIBLE_DEVICES=0,1 swift deploy --model_type qwen2 --model /mnt/suke/LLM/model/Qwen2.5-1.5B-Instruct/ --infer_backend vllm
```

出现如下内容，表示成功：

```
[INFO:swift] Start time of running main: 2025-06-12 04:02:13.785282
[INFO:swift] swift.__version__: 3.5.0
[INFO:swift] model_list: ['Qwen2.5-1.5B-Instruct']
INFO:     Started server process [119142]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
```

部署完成后，可以使用 curl 进行推理对话：

运行代码：

```
curl http://127.0.0.1:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
        "model": "Qwen2.5-1.5B-Instruct",
        "messages": [{"role": "user", "content": "你好，我头疼怎么办"}]
      }'
```

结果：

```shell
{"model":"Qwen2.5-1.5B-Instruct","choices":[{"index":0,"message":{"role":"assistant","content":"头痛是一种常见的症状，可能由多种原因引起。以下是一些缓解头痛的建议：\n\n1. 休息：确保充足的睡眠和休息时间。\n2. 饮食调整：保持均衡饮食，避免过量摄入咖啡因、酒精等刺激性物质。\n3. 水分补充：确保每天喝足够的水。\n4. 热敷或冷敷：使用热水袋或冰袋在头部疼痛区域进行热敷或冷敷。\n5. 轻松活动：适当的轻度运动如散步可以帮助放松肌肉，减轻压力。\n\n如果头痛持续存在或者伴有其他严重症状（如视力模糊、说话困难、肢体无力等），请尽快就医寻求专业医生的帮助。","tool_calls":null},"finish_reason":"stop","logprobs":null}],"usage":{"prompt_tokens":34,"completion_tokens":148,"total_tokens":182},"id":"13245636a2304399ad02c518e1fbf877","object":"chat.completion","created":1749701455}
```

部署后客户端响应

1. 使用swift的同步客户端接口：

   ```shell
   # 进入LLM目录
   cd ~/LLM
   
   # client.py程序调用LLM的api接口
   vim src/client.py
   
   python src/client.py
   ```

   `client.py`的内容如下：

   ```python
   import requests
   
   BASE_URL = "http://127.0.0.1:8000/v1/chat/completions"
   MODEL_ID = "Qwen2.5-1.5B-Instruct"
   
   def chat_once(prompt):
       headers = {"Content-Type": "application/json"}
       payload = {
           "model": MODEL_ID,
           "messages": [{"role": "user", "content": prompt}],
           "temperature": 0.7,
           "seed": 42
       }
   
       response = requests.post(BASE_URL, headers=headers, json=payload)
       if response.ok:
           result = response.json()
           print("response:", result["choices"][0]["message"]["content"])
           return result["choices"][0]["message"]["content"]
       else:
           raise RuntimeError(f"Request failed: {response.status_code}\n{response.text}")
   
   if __name__ == "__main__":
       query = "你好，我头疼怎么办？"
       chat_once(query)
   
   ```

   运行结果：

   ```shell
   response: 头疼可能是由多种原因引起的，包括压力、疲劳、脱水、眼睛疲劳等。如果你的头疼持续不断或者非常严重，你应该去看医生。
   
   在等待看医生的时间里，你可以尝试以下方法来缓解头疼：
   
   1. 休息：确保你有足够的睡眠，并尽量避免过度劳累。
   2. 水分补充：保持充足的水分摄入可以帮助缓解头疼。
   3. 轻度运动：轻度的身体活动可以促进血液循环，有助于缓解头疼。
   4. 放松技巧：深呼吸、瑜伽或冥想等放松技巧可能帮助减轻头疼。
   5. 热敷或冷敷：根据你的个人偏好，使用热敷或冷敷可以缓解头疼。
   
   但是请注意，这些只是一些缓解头疼的方法，如果头疼没有改善，或者伴有其他症状（如视力问题、恶心、呕吐等），你应该尽快寻求医疗帮助。
   
   ```

2. 使用swift的异步客户端接口：

   ```shell
   # 进入LLM目录
   cd ~/LLM
   
   # client_async.py程序调用LLM的api接口
   vim src/client_async.py
   
   python src/client_async.py
   ```

   `client_async.py`的内容如下：

   ```python
   import requests
   import json
   
   BASE_URL = "http://127.0.0.1:8000/v1/chat/completions"
   MODEL_ID = "Qwen2.5-1.5B-Instruct"
   
   def stream_chat(prompt):
       headers = {"Content-Type": "application/json"}
       payload = {
           "model": MODEL_ID,
           "stream": True,
           "temperature": 0.7,
           "messages": [
               {"role": "user", "content": prompt}
           ]
       }
   
       print(f"query: {prompt}")
       print("response: ", end='', flush=True)
   
       with requests.post(BASE_URL, headers=headers, json=payload, stream=True) as response:
           if response.status_code != 200:
               raise RuntimeError(f"Request failed: {response.status_code}\n{response.text}")
           for line in response.iter_lines():
               if line:
                   line_str = line.decode("utf-8").lstrip("data: ").strip()
                   if line_str == "[DONE]":
                       break
                   try:
                       data = json.loads(line_str)
                       delta = data["choices"][0]["delta"]
                       content = delta.get("content", "")
                       print(content, end='', flush=True)
                   except Exception as e:
                       print(f"\n[Parse Error]: {e}, line={line_str}")
       print()  # 结束换行
   
   if __name__ == "__main__":
       stream_chat("有什么药物可以治疗头疼吗?")
   
   ```

   运行结果：

   ```shell
   query: 有什么药物可以治疗头疼吗?
   response: 我不能提供具体的医疗建议或推荐特定的药物。头痛是一种常见的症状，可能由多种原因引起，包括压力、疲劳、脱水、眼睛问题或其他健康状况。
   
   如果你经常经历头痛，或者头痛影响你的日常生活，请务必咨询医生或专业的医疗人员以获得准确的诊断和适当的治疗方案。他们可能会根据你的具体情况推荐适合的药物或其他治疗方法。在没有专业意见的情况下，不要自行购买和使用任何药物，以免造成不必要的健康风险。
   ```

## 9.2 vLLM

本节将介绍如何不使用 `swift` ，而是直接通过 `vLLM` 来进行模型的部署和运行。

### 9.2.1 离线推理

在本节中，我们将仅使用 `vLLM` 来对模型进行部署和推理。以下是具体步骤：

```shell
# 进入LLM目录
cd ~/LLM

# 创建vllm_infer.py
vim src/vllm_infer.py

# 运行vllm_infer.py
python src/vllm_infer.py
```

`vllm_infer.py` 文件的内容如下所示：

```python
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

# 模型的路径
model_path = '/mnt/suke/LLM/model/Qwen2.5-1.5B-Instruct'
# 构建tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_path)
# 构建LLM
llm = LLM(model=model_path)
# 推理用参数
sampling_params = SamplingParams(temperature=0.7, top_p=0.8, repetition_penalty=1.05, max_tokens=512)
# prompt
messages = [
    {"role": "system", "content": "你是一个医疗小助手"},
]
# 运行
for query in ['我肚子有些不舒服', "请问有什么可以缓解的药物吗？"]:
    messages.append({
        'role': 'user',
        'content': query
    })
    # 使用chat模板
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    # 推理
    outputs = llm.generate([text], sampling_params)
    # 获取回答
    response = outputs[-1].outputs[0].text
    print(f'query: {query}')
    print(f'response: {response}')
    messages.append({'role': 'assistant', 'content': response})

```

运行结果如下所示：

```shell
query: 请问有什么可以缓解的药物吗？
response: 我不是医生，但我可以提供一些建议。对于轻微的腹部不适，您可以尝试使用一些非处方药物来缓解症状，比如抗酸药（如奥美拉唑）、止痛药（如布洛芬）或者消化酶补充剂（如乳酶生）。但是，请务必遵循药品说明或咨询药师以确保正确使用。

如果您的不适持续存在或者症状加重，强烈建议您尽快就医，以便获得专业的诊断和治疗。在没有专业医疗指导的情况下，请不要随意使用药物，以免造成不必要的健康风险。
```

---

![](QLUNLP_logo.png)