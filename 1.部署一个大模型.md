# 一、本地部署一个大模型！

本文将介绍如何使用SWIFT框架通过CLI与API两种方式部署Qwen2.5-0.5B-Instruct模型。

---

### 基础环境

- 操作系统：Ubuntu 22.04
- Miniconda3: conda 24.1.2
- 根据服务器网络情况配置好conda源和pip源（本案例中均使用默认源）

### 创建并配置 conda 环境

***创建并激活一个新的 conda 环境***

```bash
conda create --name swift python=3.10
conda activate swift
```

***配置 conda 环境***

激活"swift"环境后，需要安装本节所需的各类包，下文将就几个较为主要的包进行介绍。



*安装 pytorch*

如果直接安装swift，安装过程中由于没有安装pytorch，会自动下载最新版本的pytorch，而pytorch安装必须与系统CUDA版本对应；为了避免出现这种情况，因此需要提前安装pytorch与CUDA。

要安装pytorch，首先要系统所支持的CUDA版本；使用`nvidia-smi`查看系统支持的最高CUDA版本：

![](pic\nvidia-smi.png)

> 如果出现诸如`'nvidia-smi' 不是内部或外部命令，也不是可运行的程序`之类的问题，可能是没有安装nvidia显卡驱动~~或者没有N卡~~导致，请自行检索解决办法。

再得到系统支持的CUDA版本后，就可以根据系统环境安装所需要的CUDA与PyTorch版本；此部分请自行检索其他教程。

其中，CUDA版本建议为11.8、12.6或12.8（在低于支持的最高CUDA版本的情况下），PyTorch版本不得低于2.0.0（若地低于2.0.0则无法使用swift 3.x版本）



*安装 swift*

这里提供两种安装swift的方法：

Wheel包安装

```bash
pip install 'ms-swift'
```

源代码安装

```bash
git clone https://github.com/modelscope/ms-swift.git
cd ms-swift
pip install -e .
```

如果你安装后无法正常使用，这里附上swift3.X版本的**运行环境**，支持的硬件与可选依赖等更多信息请参阅[swift官方文档](https://swift.readthedocs.io/zh-cn/latest/index.html)。

|              | 范围         | 推荐   | 备注                        |
| ------------ | ------------ | ------ | --------------------------- |
| python       | >=3.9        | 3.10   |                             |
| cuda         |              | cuda12 | 使用cpu、npu、mps则无需安装 |
| torch        | >=2.0        |        |                             |
| transformers | >=4.33       | 4.51   |                             |
| modelscope   | >=1.23       |        |                             |
| peft         | >=0.11,<0.16 |        |                             |
| trl          | >=0.13,<0.18 | 0.17   | RLHF                        |
| deepspeed    | >=0.14       | 0.14.5 | 训练                        |
| vllm         | >=0.5.1      | 0.8    | 推理/部署/评测              |
| lmdeploy     | >=0.5        | 0.8    | 推理/部署/评测              |
| evalscope    | >=0.11       |        | 评测                        |



***下载 Qwen2.5-0.5B-Instruct 模型***

以下提供三种方法下载模型：

*命令行下载*

在下载前，请先通过如下命令安装ModelScope

```bash
pip install modelscope
```

下载完整模型库

```bash
modelscope download --model Qwen/Qwen2.5-0.5B-Instruct
```

下载单个文件到指定本地文件夹（以下载README.md到当前路径下“dir”目录为例）

```bash
modelscope download --model Qwen/Qwen2.5-0.5B-Instruct README.md --local_dir ./dir
```

*SDK下载*

```bash
#模型下载
from modelscope import snapshot_download
model_dir = snapshot_download('Qwen/Qwen2.5-0.5B-Instruct')
```

*Git下载*

请确保 lfs 已经被正确安装

```bash
# 安装lfs
git lfs install
# 如果lfs无法安装，可以尝试使用这个命令
sudo apt install git-lfs

#下载模型
git clone https://www.modelscope.cn/Qwen/Qwen2.5-0.5B-Instruct.git
```



## 大模型部署

### 部署 CLI 服务

> **CLI 部署**（Command Line Interface 部署）是指通过命令行来使用和管理模型。使用 CLI 部署时，用户可以在终端或命令行界面中输入命令来执行模型的各种任务（例如训练、推理、评估等）。CLI 部署一般适合个人开发和测试使用，通常在本地或服务器上执行模型操作，不需要额外的网络接口。

执行下述代码部署CLI服务：

```bash
CUDA_VISIBLE_DEVICES=0 swift infer --stream true --model <mdoel_path_or_id>
```

其中，`CUDA_VISIBLE_DEVICES`是cuda可见的显卡编号，可以指定GPU进行部署；`stream`表示是否启用流式输出；`model`是模型的类型或模型下载后保存的目录。 swift支持的模型类型可以在[这里](https://swift.readthedocs.io/zh-cn/latest/Instruction/%E6%94%AF%E6%8C%81%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86.html)查看。

指令命令后，提示一下信息则部署成功：

```shell
[INFO:swift] request_config: RequestConfig(max_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stop=[], seed=None, stream=False, logprobs=False, top_logprobs=None, n=1, best_of=None, presence_penalty=0.0, frequency_penalty=0.0, length_penalty=1.0)
[INFO:swift] Input `exit` or `quit` to exit the conversation.
[INFO:swift] Input `multi-line` to switch to multi-line input mode.
[INFO:swift] Input `reset-system` to reset the system and clear the history.
[INFO:swift] Input `clear` to clear the history.
```

现在我们可以使用终端进行推理：

```shell
<<< 你觉得你是职业选手吗？
作为AI模型，我并没有职业身份。我是由阿里云开发的超大规模语言模型，旨在为用户提供高质量、实用性的信息和帮助。我的设计目的是为了更好地服务于用户，提供有用的信息和解决问题的方法。如果您有任何问题或需要帮助，请随时告诉我，我会尽力为您提供支持和解答。
--------------------------------------------------
<<< 我的肚子有点疼
很抱歉听到您肚子疼痛的消息。请尽快联系您的医生或医疗专业人士寻求建议和治疗。在等待医疗援助期间，您可以采取一些措施来缓解不适：

1. **休息**：确保有足够的休息时间，避免过度劳累。
2. **饮食调整**：如果可能的话，尝试吃一些容易消化的食物，如白粥、煮熟的蔬菜等。
3. **保持水分**：喝足够的水可以帮助缓解疼痛，并保持身体的水分平衡。

请记住，这些建议不能替代专业医疗意见。如果症状持续或者加重，请务必及时就医。希望您早日康复！
--------------------------------------------------

```



### 部署 API 服务

> **API 部署**（Application Programming Interface 部署）是指通过 REST API 或 gRPC 接口来调用模型。这种部署方式通常会将模型作为一个服务（service）进行运行，供其他应用程序或用户通过网络请求来调用。这种方式通常在生产环境中使用，便于与其他服务进行集成，实现自动化和大规模调用。

***启动服务端***

执行下述代码部署API服务：

```bash
CUDA_VISIBLE_DEVICES=0 swift deploy --model <model_path_or_id>
```

执行命令后，提示以下信息则部署成功：

```shell
INFO:     Started server process [269651]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
```



***调用客户端***

*使用 curl*

想要快速测试API的可用性，可以使用`curl`来调用接口，其通过 HTTP 请求将输入发送给远程服务器的模型 API 端点，并返回结果：

```shell
curl http://localhost:8000/v1/chat/completions \
-H "Content-Type: application/json" \
-d '{
"model": "Qwen2.5-0.5B-Instruct", # 与model中目录名称保持一致
"messages": [{"role": "user", "content": "我有十二指肠溃疡，请问我需要吃什么药物？"}],
"max_tokens": 1024,
"temperature": 0
}'
```

可以得到下述相应：

```shell
{"model":"Qwen2.5-0.5B-Instruct","choices":[{"index":0,"message":{"role":"assistant","content":"十二指肠溃疡是一种常见的消化系统疾病，治疗上通常包括以下几个方面：\n\n1. **药物治疗**：\n   - **质子泵抑制剂（PPIs）**：如奥美拉唑、兰索拉唑等，可以减少胃酸分泌，减轻溃疡的疼痛和炎症。\n   - **H2受体拮抗剂**：如雷尼替丁、法莫替丁等，也可以帮助减少胃酸分泌。\n   - **抗生素**：如果伴有幽门螺杆菌感染，可能需要使用抗生素进行根除治疗。\n\n2. **生活方式调整**：\n   - 饮食上应避免辛辣、油腻、酸性食物以及咖啡因和酒精等刺激性饮料。\n   - 保持规律的生活作息，避免过度劳累。\n   - 戒烟限酒，因为烟草和酒精都可能加重病情。\n\n3. **定期复查**：\n   - 每半年至一年进行一次胃镜检查，以监测溃疡愈合情况及是否有复发迹象。\n\n4. **心理调适**：\n   - 保持良好的心态，避免情绪波动过大，有助于疾病的恢复。\n\n请注意，以上建议仅供参考。在采取任何治疗措施之前，强烈建议您咨询专业的医生或医疗专家，以便获得针对您个人情况的专业指导。","tool_calls":null},"finish_reason":"stop","logprobs":null}],"usage":{"prompt_tokens":41,"completion_tokens":269,"total_tokens":310},"id":"chatcmpl-e083ad03d134462394d25b03533f3d32","object":"chat.completion","created":1749004106}
```



*使用 openai 库*

我们也可以使用`Python`来调用接口。首先，安装`openai`库，以调用大模型接口：

```bash
pip install openai
```

然后，编写代码以调用接口；源代码如下：

```python
from openai import OpenAI

client = OpenAI(
    api_key='EMPTY',
    base_url='http://localhost:8000/v1',
)

# 获取模型 ID
model_type = client.models.list().data[0].id
print(f'model_type: {model_type}')

messages = []

for query in ['你是谁？', "what's your name?", '你是谁研发的？']:
   messages.append({
       'role': 'user',
       'content': query
   })
   resp = client.chat.completions.create(
       model=model_type,
       messages=messages,
       seed=42)
   response = resp.choices[0].message.content
   print(f'query: {query}')
   print(f'response: {response}')
   messages.append({'role': 'assistant', 'content': response})

# 流式输出
for query in ['78654+657=?', '晚上睡不着觉怎么办']:
    messages.append({'role': 'user', 'content': query})

    stream_resp = client.chat.completions.create(
        model=model_type,
        messages=messages,
        stream=True,
        seed=42,
    )

    print(f'\nquery: {query}')
    print('response: ', end='')
    response = ''
    # 逐个 chunk 处理并打印
    for chunk in stream_resp:
        response += chunk.choices[0].delta.content
        print(chunk.choices[0].delta.content, end='', flush=True)
    messages.append({'role': 'assistant', 'content': response})

```

> 在使用 OpenAI库调用API时，流式（streaming）和非流式（non-streaming）代表两种不同的响应数据传输方式。它们主要影响数据返回的速度和方式，适用于不同的使用场景。**非流式**调用是指在请求完成后，OpenAI API 一次性返回整个响应。也就是说，服务器处理完所有数据后，才将生成的结果整体发送回客户端。**流式**调用是指 API 按照生成内容的顺序逐步返回响应。服务器在生成每一段内容后立即传输给客户端，而不是等待所有内容生成完毕。这类似于实时流媒体的传输方式。流式调用能够显著提升实时交互体验，适合需要即时反馈的场景；而非流式调用实现更简单，适合生成较短文本或对响应速度要求不高的任务。选择哪种方式取决于具体应用场景的需求。

最后，我们运行此文件，运行结果如下：

```shell
model_type: Qwen2.5-0.5B-Instruct
query: 你是谁？
response: 我是Qwen，一个由阿里云开发的大规模语言模型。我被设计用于回答问题、创作文字和撰写代码等任务。我的目标是提供准确、有用的信息，并在需要时执行特定的任务。如果您有任何问题或需要帮助，请随时告诉我！
query: what's your name?
response: I am Qwen, a large language model developed by Alibaba Cloud. I am designed to answer questions, write text, and create code tasks. My goal is to provide accurate information when needed and execute specific tasks as required. If you have any questions or need help, feel free to ask!
query: 你是谁研发的？
response: 我是由阿里巴巴集团研发的大规模语言模型。我通过大量的数据训练和优化，能够理解和生成各种类型的文本，包括但不限于文学、科学、技术、历史、哲学等领域的内容。我的目的是为用户提供高效、高质量的答案和服务。如果您有其他关于我的信息需求，我会尽力回答。

query: 78654+657=?
response: 78654 + 657 = 79311
query: 晚上睡不着觉怎么办
response: 夜晚感到困倦是很常见的现象，这可能是由于多种原因造成的，比如压力大、睡眠不足或者环境因素（如光线、温度）影响了你的睡眠质量。以下是一些建议来帮助你改善睡眠：

1. **建立规律的作息时间**：尽量每天在同一时间上床睡觉和起床。
2. **创造良好的睡眠环境**：确保卧室安静、黑暗且温度适宜（大约16-18摄氏度）。使用遮光窗帘，减少噪音干扰。
3. **限制白天的小憩时间**：如果你白天已经感到疲倦，尝试在睡前避免长时间的小憩。
4. **放松身心**：睡前进行一些放松活动，如阅读、听轻音乐或冥想。
5. **避免咖啡因和酒精**：尤其是在下午和晚上，这些物质可能会干扰你的睡眠。
6. **适量运动**：定期进行适度的体育锻炼可以帮助减轻压力并提高睡眠质量。

如果上述方法仍然无法改善你的睡眠状况，建议咨询医生或专业的睡眠专家以获取更个性化的指导和支持。
```



*swift 客户端*

也可以使用swift客户端编写代码；源代码如下：

```python
from swift.llm import InferRequest, InferClient, RequestConfig
from swift.plugin import InferStats

engine = InferClient(host='localhost', port=8001)
print(f'models: {engine.models}')
metric = InferStats()

# 直接输出
request_config = RequestConfig(max_tokens=512, temperature=0)
for query in ['你是谁？', "what's your name?", '你是谁研发的？']:
    infer_requests = [(
        InferRequest(messages=[{'role': 'user', 'content': query}])
    )]

    response = engine.infer(infer_requests, request_config, metrics=[metric])
    print(f'query: {query}')
    print(f'response: {response[0].choices[0].message.content}')

print(metric.compute())
metric.reset()

# 流式输出
request_config = RequestConfig(max_tokens=512, temperature=0, stream=True)
for query in ['78654+657=?', '晚上睡不着觉怎么办']:
    infer_requests = [
        InferRequest(messages=[{'role': 'user', 'content': query}])
    ]

    stream_response = engine.infer(infer_requests, request_config, metrics=[metric])

    print(f'\nquery: {query}')
    print('response: ', end='')

    for chunk in stream_response[0]:
        if chunk is None:
            continue
        print(chunk.choices[0].delta.content, end='', flush=True)

print(metric.compute())
metric.reset()
```

运行结果如下：

```sh
models: ['Qwen2.5-0.5B-Instruct']
query: 你是谁？
response: 我是Qwen，一个由阿里云开发的超大规模语言模型。我被设计用于回答问题、创作文字和生成文本。我的目标是帮助用户理解和创造有意义的内容。如果您有任何问题或需要帮助，请随时告诉我！
query: what's your name?
response: I am Qwen, also known as Qwen-260B, an artificial intelligence language model developed by Alibaba Cloud. My name is derived from the Chinese character "Qwen," which means "wisdom" or "knowledge." I was trained on vast amounts of text data to understand and generate human-like responses.
query: 你是谁研发的？
response: 我是由阿里云自主研发的大规模语言模型，我叫通义千问。
{'num_prompt_tokens': 100, 'num_generated_tokens': 133, 'num_samples': 3, 'runtime': 8.507041163276881, 'samples/s': 0.35264905181726086, 'tokens/s': 15.634107963898565}

query: 78654+657=?
response: 78654 + 657 = 79311
query: 晚上睡不着觉怎么办
response: 晚上睡觉时感到困倦，这是很常见的现象。以下是一些可能有助于改善睡眠质量的建议：
1. **保持规律的作息时间**：尽量每天在相同的时间上床和起床，这有助于调整你的生物钟。
2. **创造良好的睡眠环境**：确保你的卧室安静、黑暗和凉爽。使用舒适的床上用品，并考虑安装耳塞或白噪音机来帮助掩盖干扰声。
3. **避免刺激性饮料和食物**：咖啡因和酒精可能会延迟入睡，而辛辣的食物也可能影响睡眠质量。尝试在睡前几小时避免这些食物和饮品。
4. **放松身心**：睡前进行一些放松活动，如阅读、冥想或深呼吸练习，可以帮助减轻压力和焦虑，从而更容易入睡。
5. **限制晚间屏幕时间**：电子设备发出的蓝光会抑制褪黑激素的产生，这是一种自然的生物节律调节剂，有助于提高睡眠质量。尽量减少在睡前一小时内使用手机、电脑等电子设备。
6. **适量运动**：定期的身体活动可以提高睡眠质量，但请避免在睡前进行剧烈运动。
7. **注意饮食健康**：晚餐不要吃得过饱，也不要吃得太油腻或者含糖量高的食物，因为它们可能会导致消化不良，进而影响睡眠。
如果以上方法都不能有效改善你的睡眠问题，或者你经常感到极度疲劳且无法入睡，那么寻求专业的医疗帮助可能是必要的。医生可能会推荐一些药物治疗或其他治疗方法。记住，每个人的情况都是独特的，找到最适合自己的解决方案是关键。
{'num_prompt_tokens': 74, 'num_generated_tokens': 340, 'num_samples': 2, 'runtime': 25.73147734766826, 'samples/s': 0.0777258131345201, 'tokens/s': 13.213388232868416}

```

---

![](QLUNLP_logo.png)

