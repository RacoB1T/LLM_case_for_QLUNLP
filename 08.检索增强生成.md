# 八、检索增强生成

本文将介绍如何使用LangChain构建一个非常简单的RAG应用。

---

## 基于LangChain构建RAG应用

### 环境准备

- 模型：Qwen2.5-0.5B-Instruct
- 嵌入模型：bge-m3
- 检索数据：[《GB/T *16751.2-2021中医临床诊疗术语* *第2部分:证候*》](http://www.baidu.com/link?url=VZ0NDFFJMGBVV0WvChWf7VXvVQDeeC8imOPHnYhEAW6Gck88xXAU9D9nBfW6DuMRt7RHXlogfQETGUt9I0cBDMDssdJFKUj8I8bcKwJMrvS)



### LangChain

`LangChain`官网提供了入门教程，课程地址https://python.langchain.com/docs/tutorials/

本节将简要介绍 `LangChain` 的使用方法。

1. 使用`LangChain`运行LLM：

   ```shell
   # 安装langchain和langchain-openai
   # 先启动环境
   conda activate Qwen
   # 安装必要的库
   pip install langchain
   pip install langchain-openai
   pip install langgraph
   pip install langchain-community
   pip install langchain-chroma
   pip install langchain-core
   pip install langchain-huggingface
   ```

2. 创建`rag`文件夹并写入第一个程序：

   ```shell
   # 进入LLM文件夹
   cd ~/LLM
   # 创建rag文件夹
   mkdir rag
   # 创建第一个程序quickstart.py
   vim rag/quickstart.py
   ```

   `quickstart.py`文件内容如下：

   ```python
   # LLM 创建
   from langchain_openai import ChatOpenAI
   # 输出解析
   from langchain_core.output_parsers import StrOutputParser
   # 对话模板
   from langchain_core.prompts import ChatPromptTemplate
   
   # 这是使用接口的形式调用LLM
   llm = ChatOpenAI(
       model="Qwen2.5-0.5B-Instruct",
       temperature=0,
       max_tokens=None,
       timeout=None,
       max_retries=2,
       api_key="EMPTY",
       base_url="http://localhost:8000/v1",
   )
   # 创建一个输出的解析器
   # 模型的响应是 AIMessage。它包含一个字符串响应以及有关响应的其他元数据。通常我们可能只想处理字符串响应。我们可以使用简单的输出解析器解析出此响应。
   parser = StrOutputParser()
   
   # 创建一个prompt的各个字段
   # PromptTemplates 是 LangChain 中的一个概念，旨在帮助进行此转换。它们接收原始用户输入并返回准备传递到语言模型的数据（提示）。
   task = '医疗问答'
   system_template = (
       "你是问答任务的助理。"
       "如果你不知道答案，就说你不知道。"
   )
   query = '不耐疲劳，口燥、咽干可能是哪些证候？'
   
   # prompt构建器
   prompt_template = ChatPromptTemplate.from_messages(
       [("system", system_template), ("user", "{query}")]
   )
   
   # 构建一个工作链
   chain = prompt_template | llm | parser
   
   # 运行工作链
   res = chain.invoke({"task": task, "query": query})
   
   # 打印输出
   print(res)
   
   # 流式输出
   # for chunk in chain.stream({"task": task, "query": query}):
   #     print(chunk, end="", flush=True)
   ```

   在 `LangChain` 中，`|` 运算符用于组合两个元素，`invoke` 是一个标准接口，包含以下几种方法：

   `invoke`是一个标准接口。标准接口包括：

   *   `stream`：流式返回响应的块
   *   `invoke`：在输入上调用链
   *   `batch`：在输入列表上调用链

   这些方法也有相应的异步方法，如：

   *   `astream`：异步流式返回响应的块
   *   `ainvoke`：异步在输入上调用链
   *   `abatch`：异步在输入列表上调用链
   *   `astream_log`：除了最终响应外，还流式返回发生的中间步骤。
   *   `astream_events`：流式传输链中发生的事件

   **输入类型**和**输出类型**因组件而异

   | 组件       | 输入类型                               | 输出类型     |
   | ---------- | -------------------------------------- | ------------ |
   | 提示       | 字典                                   | PromptValue  |
   | 聊天模型   | 单个字符串、聊天消息列表或 PromptValue | ChatMessage  |
   | 大语言模型 | 单个字符串、聊天消息列表或 PromptValue | 字符串       |
   | 输出解析器 | 大语言模型或聊天模型的输出             | 取决于解析器 |
   | 检索器     | 单个字符串                             | 文档列表     |
   | 工具       | 单个字符串或字典，取决于工具           | 取决于工具   |

   所有可运行对象都公开输入和输出模式以检查输入和输出

   *   `input_schema`：从可运行对象的结构自动生成的输入 Pydantic 模型
   *   `output_schema`：从可运行对象的结构自动生成的输出 Pydantic 模型

3. 运行第一个程序

   首先，需要部署一个大模型的 API 服务：

   ```shell
   # 部署LLM（这里我们选用Qwen2.5-0.5b-instruct作为部署的LLM）
   CUDA_VISIBLE_DEVICES=0 swift deploy --model Qwen2.5-0.5B-Instruct
   ```

   部署成功后，将显示如下信息：

   ```shell
   INFO: 2024-11-10 20:16:16,054 infer.py:244] system: You are Qwen, created by Alibaba Cloud. You are a helpful assistant.
   INFO:     Started server process [2000564]
   INFO:     Waiting for application startup.
   INFO:     Application startup complete.
   INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
   ```

   其中，`0.0.0.0` 表示对所有 IP 开放服务，`8000` 是 API 服务运行的端口。

   接下来，运行以下代码：

   ```shell
   # 运行第一个程序
   python rag/quickstart.py
   ```

   程序的输出结果如下所示：

   ```shell
   口燥咽干多为阴虚火旺证。阴虚火旺者，其舌质红绛，少苔或无苔，脉细数。阴虚火旺可导致体内津液耗损，出现口渴、咽干等症状。此外，还可能伴有潮热盗汗、颧红、失眠多梦等表现。因此，在辨证施治时，需要根据具体症状和体质来确定病因，并采取相应的治疗方法。
   ```
   
   LLM运行窗口的日志：
   
   ```shell
   [INFO:swift] {'response': '口燥咽干多为阴虚火旺证。阴虚火旺者，其舌质红绛，少苔或无苔，脉细数。阴虚火旺可导致体内津液耗损，出现口渴、咽干等症状。此外，还可能伴有潮热盗汗、颧红、失眠多梦等表现。因此，在辨证施治时，需要根据具体症状和体质来确定病因，并采取相应的治疗方法。', 'infer_request': {'messages': [{'content': '你是问答任务的助理。如果你不知道答案，就说你不知道。', 'role': 'system'}, {'content': '不耐疲劳，口燥、咽干可能是哪些证候？', 'role': 'user'}], 'images': [], 'audios': [], 'videos': [], 'tools': None, 'objects': {}}, 'generation_config': GenerationConfig({'bos_token_id': 151643, 'eos_token_id': [151645, 151643], 'max_new_tokens': 32727, 'pad_token_id': 151643, 'repetition_penalty': 1.1, 'return_dict_in_generate': True, 'stop_words': ['<|endoftext|>', '<|im_end|>'], 'top_logprobs': None})}
   ```



### 检索增强生成

> 检索增强生成（RAG）是一种通过引入额外信息来增强大型语言模型（LLM）知识的技术。LLM的推理能力受到其训练数据的限制，且通常只能基于截止到特定时间点的公开数据进行推理。若要开发能够推理私有数据或模型截止时间之后数据的AI应用，必须通过检索特定信息并将其整合到模型提示中，来补充模型的知识库。这一过程被称为检索增强生成（RAG）。
>
> 本节将介绍如何利用RAG技术，在非结构化文本数据上辅助LLM完成问答任务。
>
> RAG的主要流程包括：索引、检索和生成。
>
> 1. 索引
>
>    使用文本分割器将文档分割成较小的片段，然后使用嵌入模型对这些文本片段进行嵌入，并将它们存储在一个方便后续检索的向量数据库中。
>
> 2. 检索
>
>    当用户提出问题时，检索器将从存储的向量数据库中检索与问题相关的文本片段。
>
> 3. 生成
>
>    LLM将根据用户的提问和检索到的文本片段生成最终答案。

**文档准备**

本文使用的文档是由国家卫生健康委员和会国家中医药管理局发布的**中医临床诊疗术语 第2部分：证候**。其部分内容展示如下：

```json
3.5.5.5
    湿浊蒙窍证  syndrome/pattern of dampness-turbidity clouding orifices
    因湿浊壅盛，上蒙清窍所致。临床以头重闷胀，眩晕欲仆，恶心，呕吐唾沫，胸闷，舌苔白厚或垢腻，脉濡缓或滑，可伴见脑鸣、耳胀，听音不真，或眼球震颤，视物模糊，眼前有灰黄色暗影遮挡，或鼻塞、涕浊，不闻香臭等为特征的证候。

3.5.5.6
    湿浊上泛证  syndrome/pattern of dampness-turbidity flooding in the upper
    湿浊蒙上，泌别失职证
    湿浊蒙上证
    因湿浊内蕴，邪犯清空，泌别失职所致。临床以头晕作胀，神志昏蒙、恍惚，恶心、呕吐，面色晦滞，少尿或无尿，舌质淡，舌苔厚浊，脉沉缓，可伴见脘腹闷胀，不思饮食，皮肤干燥、瘙痒等为特征的证候。

3.5.5.7
    湿浊冲心证  syndrome/pattern of dampness-turbidity attacking heart
    因湿浊内蕴，壅阻心脉，上攻冲心，壅闭心神所致。临床以胸膺憋闷，心悸、怔忡，或神志恍惚，甚则昏昧，言语时或错乱，舌苔厚腻，脉弦或缓，可伴见心胸痹痛，面色晦滞，呕吐、不食等为特征的证候。
```

将*中医临床诊疗术语证候.txt*放入项目根目录下的*document*文件夹中。



**下载嵌入模型**

本文采用bge-m3模型，根据modelscope文档，可以通过如下指令将模型下载到指定文件夹：

```bash
modelscope download --model BAAI/bge-m3  --local_dir ./
```



**部署Qwen2.5-0.5B-Instruct模型**

在编写程序之前，需要首先部署将要使用的模型：

```bash
CUDA_VISIBLE_DEVICE=1 swift deploy --model Qwen/Qwen2.5-0.5B-Instruct
```

部署成功后，便可以开始编写RAG程序。



**编写RAG程序**

完整程序如下：

```python
from langchain_community.document_loaders import WebBaseLoader, TextLoader
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_text_splitters import RecursiveCharacterTextSplitter
import os
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
# 创建LLM接口
llm = ChatOpenAI(
    model="Qwen2.5-0.5B-Instruct",
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=2,
    api_key="EMPTY",
    base_url="http://localhost:8000/v1",
)
# 引入嵌入模型
model_name = "/home/qluai/lzy/langchain/model/bge-m3"
device = 'cuda:0'
model_kwargs = {'device': device}
hf = HuggingFaceEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs,
)
embeddings = hf
# 文本保存目录
txt_directory = './document/txt'
txt_files = os.listdir(txt_directory)
txt_file_list = []
for entry in txt_files:
    full_path = os.path.join(txt_directory, entry)
    if os.path.isfile(full_path):
        txt_file_list.append(full_path)
texts = []
for file_path in txt_file_list:
    # 加载文本数据
    loader = TextLoader(file_path, encoding='utf8')
    text = loader.load()
    # 文本分割器
    text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=1000, # 每一个块大小是1000
                    chunk_overlap=200, # 有200的覆盖
                    length_function=len,
                    is_separator_regex=False,
                    )
    # texts = text_splitter.create_documents([text])
    texts += text_splitter.split_documents(text)
# print(len(texts))
# 使用Chroma作为向量数据库
vectorstore = Chroma.from_documents(documents=texts, embedding=embeddings)
# 使用相似度作为检索方法、选取前3个片段
retriever = vectorstore.as_retriever(
    search_type="similarity", search_kwargs={"k": 3
    }
)
# 提示模板
system_prompt = (
    "你是问答任务的助理。"
    "使用以下检索到的上下文来回答问题。如果你不知道答案，就说你不知道。"
    "\n\n"
    "{context}"
)
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("user", "{query}"),
    ]
)
# 格式化检索的片段文本
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)
# 空文本（用作对比）
def empty_context(_):
    return ""
rag_chain = (
    # 解析输入数据
    {"context": retriever | format_docs, "query": RunnablePassthrough()}
    # {"context": empty_context, "query": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)
res = rag_chain.invoke("不耐疲劳，口燥、咽干可能是哪些证候？")
# print('============================')
print(res)

```

运行程序，大模型输出结果如下：

```sh
根据提供的信息，不耐疲劳和口燥、咽干可能是由多种原因引起的症状，包括但不限于：

1. **脾虚**：脾主统血，如果脾虚，则可能导致血液运行不畅，出现口燥、咽干等症状。
2. **肾虚**：肾藏精，主水，肾虚则可能表现为身体虚弱、容易疲劳、口干舌燥等症状。
3. **肝火旺盛**：肝火旺盛会导致体内湿热，影响津液的生成，从而导致口燥、咽干。
4. **肺热炽盛**：肺部功能失调，如肺热咳嗽、肺燥津伤等情况也可能导致口燥、咽干。
5. **胃热上炎**：胃热上炎则可能通过口腔黏膜传导至全身，引起口燥、咽干的症状。

这些症状需要结合具体情况进行综合判断，并在医生指导下进行治疗。
```

这时查看LLM的日志，可以看到与问题相关联的文本被一并输入到大模型中：

```json
[INFO:swift] {'response': '根据提供的信息，不耐疲劳和口燥、咽干可能是由多种原因引起的症状，包括但不限于：\n\n1. **脾虚**：脾主统血，如果脾虚，则可能导致血液运行不畅，出现口燥、咽干等症状。\n2. **肾虚**：肾藏精，主水，肾虚则可能表现为身体虚弱、容易疲劳、口干舌燥等症状。\n3. **肝火旺盛**：肝火旺盛会导致体内湿热，影响津液的生成，从而导致口燥、咽干。\n4. **肺热炽盛**：肺部功能失调，如肺热咳嗽、肺燥津伤等情况也可能导致口燥、咽干。\n5. **胃热上炎**：胃热上炎则可能通过口腔黏膜传导至全身，引起口燥、咽干的症状。\n\n这些症状需要结合具体情况进行综合判断，并在医生指导下进行治疗。', 'infer_request': {'messages': [{'content': '你是问答任务的助理。使用以下检索到的上下文来回答问题。如果你不知道答案，就说你不知道。\n\n干 ，潮热 、颧红等为特征的证候 。\n５．３．３．２．３．４\n阴虚咽喉失濡证 狊狔狀犱狉狅犿犲／狆犪狋狋犲狉狀狅犳狊犮犪狀狋狔犿狅犻狊狋狌狉犲犻狀狋犺狉狅犪狋犱狌犲狋狅狔犻狀犱犲犳犻犮犻犲狀犮狔\n因阴液亏虚 ，咽喉失养所致 。 临床以咽喉灼热微痛 ，或微红 、潮红 ，或有异物感 ，或局部糜烂 ，声\n音嘶哑 ，舌质红而干 ，舌苔少 ，脉细数 ，伴见咽燥 、干痒 ，口渴 、喜饮等为特征的证候 。\n５．３．３．３\n阴精亏虚证 狊狔狀犱狉狅犿犲／狆犪狋狋犲狉狀狅犳狔犻狀犲狊狊犲狀犮犲犱犲犳犻犮犻犲狀犮狔\n泛指因先天禀赋不足 ，或邪热耗伤阴液 ，或五志过极 、房事不节等耗损阴精所引起的一类证候 。\n５．３．３．３．１\n阴虚精亏证 狊狔狀犱狉狅犿犲／狆犪狋狋犲狉狀狅犳狔犻狀犱犲犳犻犮犻犲狀犮狔犪狀犱犲狊狊犲狀犮犲犱犲狆犾犲狋犻狅狀\n阴精不足证\n因先天禀赋不足 ，或久病虚损 、房事不节等耗损阴精所致 。 临床以形体消瘦 ，腰膝酸软 ，头晕 ，耳\n鸣 ，记忆力减退 ，阳痿 、精少 ，闭经 ，不孕 、不育 ，舌质红 ，舌苔少 ，脉虚细 ，伴见五心烦热 ，盗汗 ，口干 ，咽\n燥等为特征的证候 。\n５．３．３．３．２\n真阴亏虚证 狊狔狀犱狉狅犿犲／狆犪狋狋犲狉狀狅犳犵犲狀狌犻狀犲狔犻狀犱犲犳犻犮犻犲狀犮狔\n真阴不足证\n因先天禀赋不足 ，或久病虚损 ，耗伤真阴 ，阴不制阳所致 。 临床以未老先衰 ，身形矮小 ，羸瘦 、干\n枯 ，肌肤甲错 ，燥渴 、咽干 ，齿摇 、发落 ，五心烦热 ，盗汗 ，腰膝无力 ，舌体瘦小 ，舌质红或绛 ，有裂纹或面\n１４６\n\n怠 ，舌质淡 ，脉缓弱 ，伴见面色萎黄 ，头晕 、乏力 ，消瘦等为特征的证候 。\n６．６．３．１．３．２\n脾胃气阴两虚证 狊狔狀犱狉狅犿犲／狆犪狋狋犲狉狀狅犳犱狌犪犾狇犻犪狀犱狔犻狀犱犲犳犻犮犻犲狀犮狔狅犳狋犺犲狊狆犾犲犲狀犪狀犱狊狋狅犿犪犮犺\n脾胃气阴两亏证\n因气阴亏虚 ，脾胃失养所致 。 临床以脘腹痞闷或隐痛 ，饥不欲食 ，口渴 ，大便时秘时溏 ，精神萎\n靡 ，四肢无力 ，舌质淡红 ，舌苔薄白或花剥 ，脉缓弱或细 ，可伴见形体消瘦 ，呕哕 ，呃逆等为特征的\n证候 。\n６．６．３．１．３．３\n脾胃阴虚证 狊狔狀犱狉狅犿犲／狆犪狋狋犲狉狀狅犳狔犻狀犱犲犳犻犮犻犲狀犮狔犻狀狊狆犾犲犲狀犪狀犱狊狋狅犿犪犮犺\n脾胃虚热证\n因阴液亏虚 ，脾胃失润所致 。 临床以脘腹痞胀或隐痛 ，嘈杂 ，饥不欲食 ，或干哕 、呃逆 ，舌质红而\n少津 ，舌苔花剥 ，脉缓细或数 ，伴见口干 、咽燥 ，烦渴 、喜饮 ，大便干结 ，形体消瘦等为特征的证候 。\n６．６．３．１．３．４\n脾胃阳虚证 狊狔狀犱狉狅犿犲／狆犪狋狋犲狉狀狅犳狔犪狀犵犱犲犳犻犮犻犲狀犮狔犻狀狊狆犾犲犲狀犪狀犱狊狋狅犿犪犮犺\n脾胃虚寒证\n中焦虚寒证\n因脾胃阳气虚衰 ，失于温运所致 。 临床以腹胀 、食少 ，下利稀薄 ，完谷不化 ，或脘腹冷痛 ，喜温 、喜\n２２１\n\n犌犅／犜１６７５１．２—２０２１\n因津液亏损 ，形体官窍失养所致 。 临床以口干 、唇裂 ，鼻燥无涕 ，皮肤干瘪 ，目陷 、螺瘪 ，甚则肌肤\n甲错 ，舌质红而少津 ，舌中裂 ，脉细或数 ，可伴见口渴 、欲饮 ，干咳 ，目涩 ，大便干 ，小便少等为特征的\n证候 。\n５．６．１．３\n液亏证 狊狔狀犱狉狅犿犲／狆犪狋狋犲狉狀狅犳犺狌犿狅狉犱犲狆犾犲狋犻狅狀\n液伤证\n因剧烈吐泻 ，或温热邪毒等耗损阴液所致 。 临床以形体消瘦 ，眼眶凹陷 ，口干 、唇裂 ，烦渴引饮 ，\n皮肤干瘪 ，无汗 ，小便少或闭 ，大便干结 ，舌质偏红 ，舌干无津 ，脉细弱等为特征的证候 。\n５．６．１．４\n液脱证 狊狔狀犱狉狅犿犲／狆犪狋狋犲狉狀狅犳犺狌犿狅狉犮狅犾犾犪狆狊犲\n因津液极度耗损所致 。 临床以形体羸瘦 ，口唇焦裂 ，目陷 、螺瘪 ，皮肤枯瘪 ，或肌肤甲错 ，关节僵\n硬 ，屈伸不利 ，小便少或闭 ，大便燥结 ，舌质红绛 、瘦瘪 ，舌苔无津 ，脉细弱或虚数 ，或伴见眩晕 、耳鸣 ，\n头脑空痛 ，两胫酸软等为特征的证候 。\n５．６．２\n津气亏虚证 狊狔狀犱狉狅犿犲／狆犪狋狋犲狉狀狅犳犳犾狌犻犱犪狀犱狇犻犱犲狆犾犲狋犻狅狀犪狀犱犱犲犳犻犮犻犲狀犮狔\n泛指外感 、内伤损伤津气 ，甚则气随津脱等所引起的一类证候 。\n５．６．２．１\n津气两伤证 狊狔狀犱狉狅犿犲／狆犪狋狋犲狉狀狅犳犱狌犪犾犱犪犿犪犵犲狅犳犳犾狌犻犱犪狀犱狇犻\n因久病伤津耗气所致 。 临床以低热 、汗多 ，烦渴引饮 ，眼眶凹陷 ，或口燥 、咽干 ，目涩 、无泪 ，皮肤\n干燥 ，舌质红 ，舌苔干 ，脉细无力 ，伴见神疲 、乏力 ，气短 、懒言等为特征的证候 。\n５．６．２．２\n津气欲脱证 狊狔狀犱狉狅犿犲／狆犪狋狋犲狉狀狅犳犳犾狌犻犱犪狀犱狇犻狅狀狋犺犲狏犲狉犵犲狅犳犮狅犾犾犪狆狊犲\n因高热 、大汗 、严重吐泻等耗伤津气 ，气随津脱所致 。 临床以汗出不止 ，喘促欲脱 ，或二便失禁 ，\n咽干 、烦渴 ，面色苍白 ，甚则神昏 ，循衣摸床 ，撮空理线 ，脉微欲绝等为特征的证候 。\n５．６．３\n津亏兼夹证 狊狔狀犱狉狅犿犲／狆犪狋狋犲狉狀狅犳犳犾狌犻犱犲狓犺犪狌狊狋犻狅狀犮狅犿狆犾犲狓\n泛指津液亏虚 ，兼夹邪热内结或化燥 、伤阴等所引起的一类证候 。\n５．６．３．１', 'role': 'system'}, {'content': '不耐疲劳，口燥、咽干可能是哪些证候？', 'role': 'user'}], 'images': [], 'audios': [], 'videos': [], 'tools': None, 'objects': {}}, 'generation_config': GenerationConfig({'bos_token_id': 151643, 'eos_token_id': [151645, 151643], 'max_new_tokens': 30299, 'pad_token_id': 151643, 'repetition_penalty': 1.1, 'return_dict_in_generate': True, 'stop_words': ['<|endoftext|>', '<|im_end|>'], 'top_logprobs': None})}
```

与上节中不适用RAG的大模型输出结果进行对比，不难看出效果显著。



*内置链的使用* ：`langchain`库还提供了内置的链式操作来实现RAG功能。通过`create_stuff_documents_chain`和`create_retrieval_chain`，可以更简单地构建RAG流程。

- `create_stuff_documents_chain`：用于将检索到的上下文直接提供给提示和LLM，不进行任何摘要处理。
- `create_retrieval_chain`：将检索步骤和生成步骤结合在一起，输入包括`input`，输出包含`query`、`context`和`answer`。

以下是简化后的代码：

```python
from langchain_community.document_loaders import WebBaseLoader, TextLoader
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_text_splitters import RecursiveCharacterTextSplitter
import os
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
llm = ChatOpenAI(
    model="qwen2_5-1_5b-instruct",
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=2,
    api_key="EMPTY",
    base_url="http://localhost:8000/v1",
)
model_name = "./model/bge-large-zh-v1.5"
device = 'cuda:0'
model_kwargs = {'device': device}
hf = HuggingFaceEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs,
)
embeddings = hf
txt_directory = './document/txt'
txt_files = os.listdir(txt_directory)
txt_file_list = []
for entry in txt_files:
    full_path = os.path.join(txt_directory, entry)
    if os.path.isfile(full_path):
        txt_file_list.append(full_path)
texts = []
for file_path in txt_file_list:
    loader = TextLoader(file_path, encoding='utf8')
    text = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=1000, 
                    chunk_overlap=200,
                    length_function=len,
                    is_separator_regex=False,
                    )
    # texts = text_splitter.create_documents([text])
    texts += text_splitter.split_documents(text)
vectorstore = Chroma.from_documents(documents=texts, embedding=embeddings)
retriever = vectorstore.as_retriever(
    search_type="similarity", search_kwargs={"k": 3
    }
)
system_prompt = (
    "你是问答任务的助理。"
    "使用以下检索到的上下文来回答问题。如果你不知道答案，就说你不知道。"
    "\n\n"
    "{context}"
)
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("user", "{input}"),
    ]
)
# 将检索上下文和query构成prompt送入LLM
question_answer_chain = create_stuff_documents_chain(llm, prompt)
# 检索上下文信息将检索器集成到链中
rag_chain = create_retrieval_chain(retriever, question_answer_chain)
response = rag_chain.invoke({"input": "不耐疲劳，口燥、咽干可能是哪些证候？"})
print(response["answer"])
```

### 

## 存储数据到向量数据库

在运行上文的程序时，会发现一个令人头痛的问题：在对文档进行切分，将切分后的片段转化为embedding向量，构建向量索引时，会花费大量的时间。对于这个问题，很容易就能想到可以将构建好的embedding向量和向量索引存储在文件或数据库(如Milvus向量数据库)中，然后在需要时从文件或数据库中直接读取这些数据。

为此，可以编写一个这样的代码：第一次运行时，会将数据存储到数据库中，以后每次查询时都可以直接检索。通过比较第一次和第二次程序运行的时间，可以看出两次RAG时的速度差距。

具体代码如下：

```python
import time
from langchain_community.document_loaders import TextLoader
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
import os
start = time.time()
llm = ChatOpenAI(
    model="Qwen2.5-0.5B-Instruct",
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=2,
    api_key="EMPTY",
    base_url="http://localhost:8000/v1",
)
model_name = "/home/qluai/lzy/langchain/model/bge-m3"
device = 'cuda:0'
model_kwargs = {'device': device}
hf = HuggingFaceEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs,
)
embeddings = hf
# 在第一次运行程序的时候需要建立数据库
if not os.path.exists("./doc-txt_db"):
    txt_directory = './document/txt'
    txt_files = os.listdir(txt_directory)
    txt_file_list = []
    for entry in txt_files:
        full_path = os.path.join(txt_directory, entry)
        if os.path.isfile(full_path):
            txt_file_list.append(full_path)
    texts = []
    for file_path in txt_file_list:
        loader = TextLoader(file_path, encoding='utf8')
        text = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(
                        chunk_size=1000, 
                        chunk_overlap=200,
                        length_function=len,
                        is_separator_regex=False,
                        )
        texts += text_splitter.split_documents(text)
    # 数据库创建
    vectorstore = Chroma.from_documents(
        collection_name="doc-txt",
        documents=texts, 
        embedding=embeddings,
        persist_directory="./doc-txt_db"
    )
else :
    # 之后仅需要加载数据库即可
    vectorstore = Chroma(
        collection_name="doc-txt",
        embedding_function=embeddings,
        persist_directory="./doc-txt_db"
    )

retriever = vectorstore.as_retriever(
    search_type="similarity", search_kwargs={"k": 3
    }
)
# 提示模板
system_prompt = (
    "你是问答任务的助理。"
    "使用以下检索到的上下文来回答问题。如果你不知道答案，就说你不知道。"
    "\n\n"
    "{context}"
)
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("user", "{query}"),
    ]
)
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)
def empty_context(_):
    return ""
rag_chain = (
    {"context": retriever | format_docs, "query": RunnablePassthrough()}
    # {"context": empty_context, "query": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

res = rag_chain.invoke("不耐疲劳，口燥、咽干可能是哪些证候？")
print(res)
end = time.time()
print(end-start)

```



运行两次程序，第一次与第二次运行的输出如下：

```bash
# 第一次运行
根据提供的信息，不耐疲劳和口燥、咽干可能是由多种原因引起的症状，包括但不限于：

1. **脾虚**：脾主统血，如果脾虚，则可能导致血液运行不畅，出现口燥、咽干等症状。
2. **肾虚**：肾藏精，主水，肾虚则可能表现为身体虚弱、容易疲劳、口干舌燥等症状。
3. **肝火旺盛**：肝火旺盛会导致体内湿热，影响津液的生成，从而导致口燥、咽干。
4. **肺热炽盛**：肺部功能失调，如肺热咳嗽、肺燥津伤等情况也可能导致口燥、咽干。
5. **胃热上炎**：胃热上炎则可能通过口腔黏膜传导至全身，引起口燥、咽干的症状。

这些症状需要结合具体情况进行综合判断，并在医生指导下进行治疗。
143.4352424144745

# 第二次运行
根据提供的信息，不耐疲劳和口燥、咽干可能是由多种原因引起的症状，包括但不限于：

1. **脾虚**：脾主统血，如果脾虚，则可能导致血液运行不畅，出现口燥、咽干等症状。
2. **肾虚**：肾藏精，主水，肾虚则可能表现为身体虚弱、容易疲劳、口干舌燥等症状。
3. **肝火旺盛**：肝火旺盛会导致体内湿热，影响津液的生成，从而导致口燥、咽干。
4. **肺热炽盛**：肺部功能失调，如肺热咳嗽、肺燥津伤等情况也可能导致口燥、咽干。
5. **胃热上炎**：胃热上炎则可能通过口腔黏膜传导至全身，引起口燥、咽干的症状。

这些症状需要结合具体情况进行综合判断，并在医生指导下进行治疗。
36.1005220413208
```

效果显著。

---

![QLUNLP_logo](C:\Users\racob\Desktop\大模型案例\QLUNLP_logo.png)
